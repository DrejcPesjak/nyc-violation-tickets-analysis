{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DASK_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dask client\n",
    "client = Client(n_workers=4, threads_per_worker=1, memory_limit='2GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summons Number                       string[pyarrow]\n",
      "Plate ID                             string[pyarrow]\n",
      "Registration State                   string[pyarrow]\n",
      "Plate Type                           string[pyarrow]\n",
      "Issue Date                           string[pyarrow]\n",
      "Violation Code                       string[pyarrow]\n",
      "Vehicle Body Type                    string[pyarrow]\n",
      "Vehicle Make                         string[pyarrow]\n",
      "Issuing Agency                       string[pyarrow]\n",
      "Street Code1                         string[pyarrow]\n",
      "Street Code2                         string[pyarrow]\n",
      "Street Code3                         string[pyarrow]\n",
      "Vehicle Expiration Date              string[pyarrow]\n",
      "Violation Location                   string[pyarrow]\n",
      "Violation Precinct                   string[pyarrow]\n",
      "Issuer Precinct                      string[pyarrow]\n",
      "Issuer Code                          string[pyarrow]\n",
      "Issuer Command                       string[pyarrow]\n",
      "Issuer Squad                         string[pyarrow]\n",
      "Violation Time                       string[pyarrow]\n",
      "Time First Observed                  string[pyarrow]\n",
      "Violation County                     string[pyarrow]\n",
      "Violation In Front Of Or Opposite    string[pyarrow]\n",
      "House Number                         string[pyarrow]\n",
      "Street Name                          string[pyarrow]\n",
      "Intersecting Street                  string[pyarrow]\n",
      "Date First Observed                  string[pyarrow]\n",
      "Law Section                          string[pyarrow]\n",
      "Sub Division                         string[pyarrow]\n",
      "Violation Legal Code                 string[pyarrow]\n",
      "Days Parking In Effect               string[pyarrow]\n",
      "From Hours In Effect                 string[pyarrow]\n",
      "To Hours In Effect                   string[pyarrow]\n",
      "Vehicle Color                        string[pyarrow]\n",
      "Unregistered Vehicle?                string[pyarrow]\n",
      "Vehicle Year                         string[pyarrow]\n",
      "Meter Number                         string[pyarrow]\n",
      "Feet From Curb                       string[pyarrow]\n",
      "Violation Post Code                  string[pyarrow]\n",
      "Violation Description                string[pyarrow]\n",
      "No Standing or Stopping Violation    string[pyarrow]\n",
      "Hydrant Violation                    string[pyarrow]\n",
      "Double Parking Violation             string[pyarrow]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load multiple Parquet files\n",
    "data = dd.read_parquet('parquet_data/*.parquet')\n",
    "\n",
    "# print all columns and their data types\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew99/School/BigData/project/venv-bigdproject/lib/python3.10/site-packages/dask_expr/_collection.py:4160: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('violation_county', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "# keep only the columns we need\n",
    "columns_to_keep = [\n",
    "    'Violation County',\n",
    "    'Issue Date',\n",
    "    'Violation Time',]\n",
    "\n",
    "data = data[columns_to_keep]\n",
    "\n",
    "# rename columns to remove spaces and make them lowercase\n",
    "data = data.rename(columns={\n",
    "    'Violation County': 'violation_county',\n",
    "    'Issue Date': 'issue_date',\n",
    "    'Violation Time': 'violation_time'\n",
    "})\n",
    "\n",
    "# fix the county names\n",
    "remap_county_dict = {\n",
    "    'K' : 'Brooklyn',\n",
    "    'Q' : 'Queens',\n",
    "    'NY': 'Manhattan',\n",
    "    'QN': 'Queens',\n",
    "    'BK': 'Brooklyn',\n",
    "    'R' : 'Staten Island',\n",
    "    'BX': 'Bronx',\n",
    "    'ST': 'Staten Island',\n",
    "    'MN': 'Manhattan',\n",
    "    'KINGS': 'Brooklyn',\n",
    "    'QNS': 'Queens',\n",
    "    'BRONX': 'Bronx',\n",
    "    'RICHM': 'Staten Island',\n",
    "    'RICH': 'Staten Island',\n",
    "    'Queens': 'Queens',\n",
    "    'Manhattan': 'Manhattan',\n",
    "    'Bronx': 'Bronx',\n",
    "    'Brooklyn': 'Brooklyn',\n",
    "    'Staten Island': 'Staten Island',\n",
    "    'Rich': 'Staten Island',\n",
    "    'QUEEN': 'Queens',\n",
    "    'NEW Y': 'Manhattan',\n",
    "    'NYC': 'Manhattan',\n",
    "    'USA': 'Unknown',\n",
    "    'K   F': 'Brooklyn',\n",
    "    'VINIS': 'Staten Island',\n",
    "    'NEWY': 'Manhattan',\n",
    "    'KING': 'Brooklyn',\n",
    "    'PBX': 'Bronx',\n",
    "    'MS': 'Unknown',\n",
    "    'ABX': 'Bronx',\n",
    "    '103': 'Unknown',\n",
    "    '108': 'Unknown',\n",
    "    'MH': 'Manhattan',\n",
    "    'MAN': 'Manhattan',\n",
    "    'P': 'Unknown',\n",
    "    'N': 'Unknown',\n",
    "    'A': 'Unknown',\n",
    "    'F': 'Unknown',\n",
    "    'QU': 'Queens'\n",
    "}\n",
    "data['violation_county'] = data['violation_county'].map(remap_county_dict).astype('category')\n",
    "\n",
    "# convert the Issue Date to a datetime object\n",
    "data['issue_date'] = dd.to_datetime(data['issue_date'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Remove 'A' and 'P' from the end of the time, add ' AM' or ' PM' accordingly\n",
    "data['violation_time'] = data['violation_time'].str.slice(stop=-1) + ' ' + data['violation_time'].str.slice(start=-1).replace({'A': 'AM', 'P': 'PM'})\n",
    "\n",
    "# Convert the Violation Time to a datetime object\n",
    "data['violation_time'] = dd.to_datetime(data['violation_time'], format='%I%M %p', errors='coerce')\n",
    "\n",
    "# create a new column for the day of the week the violation was issued\n",
    "data['violation_day_week'] = data['issue_date'].dt.dayofweek\n",
    "\n",
    "# create a new column for the day of the month the violation was issued\n",
    "data['violation_day_month'] = data['issue_date'].dt.day\n",
    "\n",
    "# create a new column for the month the violation was issued\n",
    "data['violation_month'] = data['issue_date'].dt.month\n",
    "\n",
    "# create a new column for the year the violation was issued\n",
    "data['violation_year'] = data['issue_date'].dt.year\n",
    "\n",
    "# keep only rows with valid year (2013-2024)\n",
    "data = data[(data['violation_year'] >= 2013) & (data['violation_year'] <= 2024)]\n",
    "\n",
    "# keep only rows with valid month (1-12)\n",
    "data = data[(data['violation_month'] >= 1) & (data['violation_month'] <= 12)]\n",
    "\n",
    "# drop all rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# create a new column for the hour of the day the violation was issued\n",
    "data['violation_hour'] = data['violation_time'].dt.hour.astype('int32')\n",
    "\n",
    "# drop the Issue Date and Violation Time columns\n",
    "data = data.drop(columns=['violation_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop county column\n",
    "data = data.drop(columns=['violation_county'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue_date             datetime64[ns]\n",
      "violation_day_week              int32\n",
      "violation_day_month             int32\n",
      "violation_month                 int32\n",
      "violation_year                  int32\n",
      "violation_hour                  int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print type of columns\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print head of the data\n",
    "# print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dataset to an hourly level - count the number of violations per hour\n",
    "data = data.groupby(['violation_year', 'violation_month', 'violation_day_month', 'violation_day_week', 'violation_hour']).size().reset_index()\n",
    "data = data.rename(columns={0: 'violation_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation_year         int32\n",
      "violation_month        int32\n",
      "violation_day_month    int32\n",
      "violation_day_week     int32\n",
      "violation_hour         int32\n",
      "violation_count        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new column of format ='%Y-%m-%d %H:%M:%S' for the datetime (for joining with weather data)\n",
    "data['datetime'] = data['violation_year'].astype(str) + '-' + data['violation_month'].astype(str).str.zfill(2) + '-' + data['violation_day_month'].astype(str).str.zfill(2) + ' ' + data['violation_hour'].astype(str).str.zfill(2) + ':00:00'\n",
    "data['datetime'] = dd.to_datetime(data['datetime'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation_year                  int32\n",
      "violation_month                 int32\n",
      "violation_day_month             int32\n",
      "violation_day_week              int32\n",
      "violation_hour                  int32\n",
      "violation_count                 int64\n",
      "datetime               datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by month and count the number of violations\n",
    "monthly_violations = data.groupby(data['datetime'].dt.to_period('M')).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the monthly violations\n",
    "# monthly_violations.compute().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the data take only the rows that have the datetime between 2013-01-01 and 2015-03-31\n",
    "data = data[(data['datetime'] >= '2013-01-01') & (data['datetime'] <= '2015-03-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation_year                  int32\n",
      "violation_month                 int32\n",
      "violation_day_month             int32\n",
      "violation_day_week              int32\n",
      "violation_hour                  int32\n",
      "violation_count                 int64\n",
      "datetime               datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking data from 2013-01-01 00:00:00 to 2015-03-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a complete range of dates and hours directly with hourly frequency\n",
    "# min_date = data['datetime'].min().compute()\n",
    "# max_date = data['datetime'].max().compute()\n",
    "min_date = '2013-01-01 00:00:00'\n",
    "min_date = pd.to_datetime(min_date)\n",
    "max_date = '2015-03-31 23:00:00'\n",
    "max_date = pd.to_datetime(max_date)\n",
    "print(f\"Taking data from {min_date} to {max_date}\")\n",
    "all_date_hours = pd.date_range(start=min_date, end=max_date, freq='h').to_frame(index=False, name='datetime')\n",
    "\n",
    "# Extract year, month, day_of_month, and day_of_week directly from the datetime\n",
    "all_date_hours['violation_year'] = all_date_hours['datetime'].dt.year\n",
    "all_date_hours['violation_month'] = all_date_hours['datetime'].dt.month\n",
    "all_date_hours['violation_day_month'] = all_date_hours['datetime'].dt.day\n",
    "all_date_hours['violation_day_week'] = all_date_hours['datetime'].dt.dayofweek\n",
    "all_date_hours['violation_hour'] = all_date_hours['datetime'].dt.hour\n",
    "\n",
    "# Step 2: Convert the comprehensive Pandas DataFrame to a Dask DataFrame\n",
    "all_date_hours_dd = dd.from_pandas(all_date_hours, npartitions=10)\n",
    "\n",
    "# Step 3: Merge with existing data using Dask's merge function\n",
    "complete_data = dd.merge(all_date_hours_dd, data, on=['violation_year', 'violation_month', 'violation_day_month', 'violation_day_week', 'violation_hour'], how='outer')\n",
    "\n",
    "# Fill missing values with 0\n",
    "complete_data['violation_count'] = complete_data['violation_count'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop datetime_y column and rename datetime_x to datetime\n",
    "complete_data = complete_data.drop(columns=['datetime_y'])\n",
    "complete_data = complete_data.rename(columns={'datetime_x': 'datetime'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime               datetime64[ns]\n",
      "violation_year                  int32\n",
      "violation_month                 int32\n",
      "violation_day_month             int32\n",
      "violation_day_week              int32\n",
      "violation_hour                  int32\n",
      "violation_count                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print columns\n",
    "print(complete_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path pattern to read all CSV files (adjust the path as needed)\n",
    "file_pattern = '../task2-aug/nyc_w_*.csv'\n",
    "\n",
    "# Specify the data types for each column\n",
    "dtypes = {\n",
    "    'datetime': 'str',  # 'datetime' will be parsed separately\n",
    "    'temp': 'float64',\n",
    "    'feelslike': 'float64',\n",
    "    'dew': 'float64',\n",
    "    'humidity': 'float64',\n",
    "    'precip': 'float64',\n",
    "    'precipprob': 'float64',\n",
    "    'preciptype': 'object',  # String data type\n",
    "    'snow': 'float64',\n",
    "    'snowdepth': 'float64',\n",
    "    'windgust': 'float64',\n",
    "    'windspeed': 'float64',\n",
    "    'winddir': 'float64',\n",
    "    'sealevelpressure': 'float64',\n",
    "    'cloudcover': 'float64',\n",
    "    'visibility': 'float64',\n",
    "    'solarradiation': 'float64',\n",
    "    'solarenergy': 'float64',\n",
    "    'uvindex': 'float64',\n",
    "    'severerisk': 'float64',\n",
    "    'conditions': 'object',  # String data type\n",
    "    'icon': 'object',  # String data type\n",
    "    'stations': 'object'  # String data type\n",
    "}\n",
    "\n",
    "# Read all CSV files into a single Dask DataFrame with specified dtypes\n",
    "df = dd.read_csv(file_pattern, dtype=dtypes)\n",
    "\n",
    "# Convert the 'datetime' column to datetime type\n",
    "df['datetime'] = dd.to_datetime(df['datetime'], format='%Y-%m-%dT%H:%M:%S', errors='coerce')\n",
    "\n",
    "# Drop all string columns except 'datetime'\n",
    "columns_to_keep = ['datetime'] + [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime            datetime64[ns]\n",
      "temp                       float64\n",
      "feelslike                  float64\n",
      "dew                        float64\n",
      "humidity                   float64\n",
      "precip                     float64\n",
      "precipprob                 float64\n",
      "snow                       float64\n",
      "snowdepth                  float64\n",
      "windgust                   float64\n",
      "windspeed                  float64\n",
      "winddir                    float64\n",
      "sealevelpressure           float64\n",
      "cloudcover                 float64\n",
      "visibility                 float64\n",
      "solarradiation             float64\n",
      "solarenergy                float64\n",
      "uvindex                    float64\n",
      "severerisk                 float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print types of the columns\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the daily data with the weather data\n",
    "augmented_data = data.merge(df, left_on='datetime', right_on='datetime', how='left')\n",
    "\n",
    "# print(augmented_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of missing values in each column\n",
    "print(augmented_data.isnull().sum().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop sewerisk and windgust column\n",
    "augmented_data = augmented_data.drop(columns=['severerisk', 'windgust'])\n",
    "\n",
    "# drop all rows with missing values\n",
    "augmented_data = augmented_data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data.to_parquet('augmented_data', write_index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd-project-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
