{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ruan.dev/blog/2023/05/17/running-a-multi-broker-kafka-cluster-on-docker\n",
    "# docker compose -f docker-compose3.yml up -d\n",
    "# docker exec -it broker-1 kafka-topics --create --topic NYTickets --bootstrap-server broker-1:29091 --partitions 3 --replication-factor 3\n",
    "\n",
    "# drew99@drew99-HPl:~/School/BigData/project/task4-kafka$ docker exec -it broker-1 kafka-topics --list --bootstrap-server broker-1:29091\n",
    "# NYTickets\n",
    "# drew99@drew99-HPl:~/School/BigData/project/task4-kafka$ docker exec -it broker-1 kafka-topics --list --bootstrap-server broker-2:29092\n",
    "# NYTickets\n",
    "# drew99@drew99-HPl:~/School/BigData/project/task4-kafka$ docker exec -it broker-1 kafka-topics --list --bootstrap-server broker-3:29093\n",
    "# NYTickets\n",
    "\n",
    "# data stored in  var/lib/docker \n",
    "# docker container prune\n",
    "# docker volume prune\n",
    "# docker network prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting producer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile producer.py\n",
    "import confluent_kafka as kafka\n",
    "import socket\n",
    "import json, os, time\n",
    "\n",
    "# producer = kafka.Producer({\n",
    "#     'bootstrap.servers': \"localhost:29092\",\n",
    "#     'client.id': socket.gethostname()\n",
    "# })\n",
    "producer = kafka.Producer({\n",
    "    'bootstrap.servers': \"localhost:9091,localhost:9092,localhost:9093\",\n",
    "    'client.id': socket.gethostname()\n",
    "})\n",
    "\n",
    "topic = \"NYTickets\"\n",
    "\n",
    "def acked(err, msg):\n",
    "    if err is not None:\n",
    "        print(\"Failed to deliver message: %s: %s\" % (str(msg), str(err)))\n",
    "    else:\n",
    "        print(\"Message produced: %s\" % (str(msg)))\n",
    "\n",
    "def send_tickets(file):\n",
    "    columns = None\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            vals = line.strip().split(\",\")\n",
    "            vals = [v.strip() for v in vals]\n",
    "            if columns is None:\n",
    "                columns = vals\n",
    "                continue\n",
    "            key = vals[0]  # Use Summons Number as key\n",
    "            msg = dict(zip(columns, vals))\n",
    "            val = json.dumps(msg)\n",
    "            producer.produce(topic, key=key, value=val, callback=acked)\n",
    "            producer.poll(0)\n",
    "            print(f\"Sent data {key}\")#:{val}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "data_dir = \"../data\"\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "for file in files:\n",
    "    send_tickets(file)\n",
    "    print(f\"Finished sending data for {file}\")\n",
    "\n",
    "producer.flush()\n",
    "print(\"Finished sending all data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting consumer_all.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile consumer_all.py\n",
    "import confluent_kafka as kafka, socket\n",
    "import os, socket, json\n",
    "from datetime import date\n",
    "\n",
    "consumer = kafka.Consumer({\n",
    "    'bootstrap.servers': \"localhost:9091,localhost:9092,localhost:9093\",\n",
    "    'client.id': socket.gethostname(),\n",
    "    'group.id': 'test_group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "})\n",
    "\n",
    "topic = \"NYTickets\"\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "window_size = 12\n",
    "sum_year, count = 0, 0\n",
    "min_y, max_y = 3000, 0\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise Exception(msg.error())\n",
    "        else:\n",
    "            # Avg Vehicle Year\n",
    "            key = msg.key().decode('utf-8')\n",
    "            val = msg.value().decode('utf-8')\n",
    "            time = msg.timestamp()[1]\n",
    "            # print(f\"Received data {key}:{val} at time {time}\")\n",
    "\n",
    "            ticket = json.loads(val)\n",
    "            year = int(ticket[\"Vehicle Year\"])\n",
    "\n",
    "            curr_year = date.today().year\n",
    "            if year < 1885 or year > curr_year: # First car was made in 1885\n",
    "                print(f\"Found invalid year {year} for ticket {key}\")\n",
    "                continue\n",
    "            \n",
    "            sum_year += year\n",
    "            count += 1\n",
    "\n",
    "            if year < min_y:\n",
    "                min_y = year\n",
    "                print(f\"Found new oldest vehicle {ticket['Vehicle Make']} {year}\")\n",
    "            if year > max_y:\n",
    "                max_y = year\n",
    "                print(f\"Found new newest vehicle {ticket['Vehicle Make']} {year}\")\n",
    "            \n",
    "            if count % window_size == 0:\n",
    "                avg_year = sum_year / window_size\n",
    "                print(f\"Received {window_size} tickets. Average Vehicle Year: {avg_year:.2f}\")\n",
    "                sum_year = 0\n",
    "                count = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consumer interrupted.\")\n",
    "finally:\n",
    "    consumer.close()\n",
    "\n",
    "print(\"Finished receiving all data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting consumer_borough.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile consumer_borough.py\n",
    "import confluent_kafka as kafka, socket\n",
    "import os, socket, json\n",
    "\n",
    "consumer = kafka.Consumer({\n",
    "    'bootstrap.servers': \"localhost:9091,localhost:9092,localhost:9093\",\n",
    "    'client.id': socket.gethostname(),\n",
    "    'group.id': 'test_group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "})\n",
    "\n",
    "topic = \"NYTickets\"\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "window_size = 12\n",
    "values = {} # key: borough, value: list of vehicle years\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise Exception(msg.error())\n",
    "        else:\n",
    "            # key = msg.key().decode('utf-8') # Summons Number\n",
    "            val = msg.value().decode('utf-8')\n",
    "            time = msg.timestamp()[1]\n",
    "\n",
    "            ticket = json.loads(val)\n",
    "            key = ticket[\"Violation County\"]\n",
    "            year = int(ticket[\"Vehicle Year\"])\n",
    "\n",
    "            if key not in values:\n",
    "                values[key] = []\n",
    "            values[key].append(year)\n",
    "\n",
    "            if len(values[key]) > window_size:\n",
    "                values[key].pop(0)\n",
    "            \n",
    "            if len(values[key]) < window_size:\n",
    "                continue\n",
    "\n",
    "            avg_year = sum(values[key]) / window_size\n",
    "            std_dev = (sum([(v - avg_year) ** 2 for v in values[key]]) / window_size) ** 0.5\n",
    "            min_year = min(values[key])\n",
    "            max_year = max(values[key])\n",
    "            \n",
    "            print(f\"Received {window_size} tickets for {key}. Average Vehicle Year: {avg_year:.2f} +/- {std_dev:.2f}. Min: {min_year}, Max: {max_year}\")\n",
    "            \n",
    "            invalid = [v for v in values[key] if v < 1885 or v > 2024]\n",
    "            if invalid:\n",
    "                print(f\"Found invalid years {invalid} for borough {key}\")\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consumer interrupted.\")\n",
    "finally:\n",
    "    consumer.close()\n",
    "\n",
    "print(\"Finished receiving all data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting consumer_street.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile consumer_street.py\n",
    "import confluent_kafka as kafka, socket\n",
    "import os, socket, json\n",
    "\n",
    "consumer = kafka.Consumer({\n",
    "    'bootstrap.servers': \"localhost:9091,localhost:9092,localhost:9093\",\n",
    "    'client.id': socket.gethostname(),\n",
    "    'group.id': 'test_group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "})\n",
    "\n",
    "topic = \"NYTickets\"\n",
    "consumer.subscribe([topic])\n",
    "\n",
    "streets = [street.strip().upper() for street in \"\"\"\n",
    "Broadway\n",
    "3rd Ave\n",
    "5th Ave\n",
    "Madison Ave\n",
    "Lexington Ave\n",
    "2nd Ave\n",
    "1st Ave\n",
    "Queens Blvd\n",
    "8th Ave\n",
    "7th Ave\n",
    "\"\"\".split(\"\\n\") if street]\n",
    "\n",
    "window_size = 12\n",
    "values = {} # key: street, value: list of vehicle years\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise Exception(msg.error())\n",
    "        else:\n",
    "            # key = msg.key().decode('utf-8')\n",
    "            val = msg.value().decode('utf-8')\n",
    "            time = msg.timestamp()[1]\n",
    "\n",
    "            ticket = json.loads(val)\n",
    "            key = ticket[\"Street\"].upper()\n",
    "            if key not in streets:\n",
    "                continue\n",
    "            year = int(ticket[\"Vehicle Year\"])\n",
    "\n",
    "            if key not in values:\n",
    "                values[key] = []\n",
    "            values[key].append(year)\n",
    "\n",
    "            if len(values[key]) > window_size:\n",
    "                values[key].pop(0)\n",
    "            \n",
    "            if len(values[key]) < window_size:\n",
    "                continue\n",
    "\n",
    "            avg_year = sum(values[key]) / window_size\n",
    "            std_dev = (sum([(v - avg_year) ** 2 for v in values[key]]) / window_size) ** 0.5\n",
    "            min_year = min(values[key])\n",
    "            max_year = max(values[key])\n",
    "\n",
    "            print(f\"Received {window_size} tickets for {key}. Average Vehicle Year: {avg_year:.2f} +/- {std_dev:.2f}. Min: {min_year}, Max: {max_year}\")\n",
    "            \n",
    "            invalid = [v for v in values[key] if v < 1885 or v > 2024]\n",
    "            if invalid:\n",
    "                print(f\"Found invalid years {invalid} for street {key}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Consumer interrupted.\")\n",
    "finally:\n",
    "    consumer.close()\n",
    "\n",
    "print(\"Finished receiving all data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile consumer_knn.py\n",
    "# import confluent_kafka as kafka\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# class VehicleYearInferenceConsumer:\n",
    "#     def __init__(self, topic, bootstrap_servers, group_id):\n",
    "#         self.consumer = kafka.Consumer({\n",
    "#             'bootstrap.servers': bootstrap_servers,\n",
    "#             'group.id': group_id,\n",
    "#             'auto.offset.reset': 'earliest'\n",
    "#         })\n",
    "#         # self.producer = kafka.Producer({\n",
    "#         #     'bootstrap.servers': bootstrap_servers\n",
    "#         # })\n",
    "#         self.topic = topic\n",
    "#         self.consumer.subscribe([topic])\n",
    "#         self.samples = defaultdict(list)\n",
    "#         self.centroids = {}\n",
    "#         self.sample_count = defaultdict(int)\n",
    "#         self.total_samples = 0\n",
    "#         self.warmup_complete = False\n",
    "#         # zero missing values columns\n",
    "#         self.columns = ['Registration State', 'Plate Type', 'Violation Code',\n",
    "#                     'Issuing Agency', 'Street Code1', 'Street Code2', 'Street Code3',\n",
    "#                     'Vehicle Expiration Date', 'Violation Precinct', 'Issuer Precinct',\n",
    "#                     'Issuer Code', 'Violation Time', 'Date First Observed', 'Law Section',\n",
    "#                     'Vehicle Year', 'Feet From Curb']\n",
    "#         # numeric columns\n",
    "#         self.columns = ['Violation Code', 'Street Code1', 'Street Code2',\n",
    "#                         'Street Code3', 'Vehicle Expiration Date', 'Violation Location',\n",
    "#                         'Violation Precinct', 'Issuer Precinct', 'Issuer Code',\n",
    "#                         'Date First Observed', 'Law Section', 'Unregistered Vehicle?',\n",
    "#                         'Vehicle Year', 'Feet From Curb']\n",
    "#         # usefull columns\n",
    "#         # ...\n",
    "\n",
    "#     def collect_samples(self, record):\n",
    "#         vehicle_year = int(record['Vehicle Year'])\n",
    "#         if vehicle_year != 0:\n",
    "#             features = self.extract_features(record)\n",
    "#             self.samples[vehicle_year].append(features)\n",
    "#             self.sample_count[vehicle_year] += 1\n",
    "#             if self.sample_count[vehicle_year] == 1000:\n",
    "#                 self.calculate_centroid(vehicle_year)\n",
    "\n",
    "#     def extract_features(self, record):\n",
    "#         features = []\n",
    "#         for column in self.columns:\n",
    "#             value = record.get(column, 0)\n",
    "#             features.append(float(value) if value else 0.0)\n",
    "#         return np.array(features)\n",
    "\n",
    "#     def calculate_centroid(self, vehicle_year):\n",
    "#         self.centroids[vehicle_year] = np.mean(self.samples[vehicle_year], axis=0)\n",
    "#         self.samples[vehicle_year] = []\n",
    "#         self.warmup_complete = True\n",
    "\n",
    "#     def update_centroid(self, vehicle_year, new_sample):\n",
    "#         if vehicle_year in self.centroids:\n",
    "#             current_centroid = self.centroids[vehicle_year]\n",
    "#             new_centroid = (0.99 * current_centroid + 0.01 * new_sample) / 1.0\n",
    "#             self.centroids[vehicle_year] = new_centroid\n",
    "\n",
    "#     def infer_vehicle_year(self, features):\n",
    "#         distances = {year: np.linalg.norm(features - centroid) for year, centroid in self.centroids.items()}\n",
    "#         return min(distances, key=distances.get)\n",
    "\n",
    "#     def evaluate_model(self):\n",
    "#         known_years = []\n",
    "#         inferred_years = []\n",
    "#         for year, samples in self.samples.items():\n",
    "#             for sample in samples[:100]:\n",
    "#                 inferred_year = self.infer_vehicle_year(sample)\n",
    "#                 known_years.append(year)\n",
    "#                 inferred_years.append(inferred_year)\n",
    "#         rmse = mean_squared_error(known_years, inferred_years, squared=False)\n",
    "#         print(f\"RMSE: {rmse}\")\n",
    "\n",
    "#     def run(self):\n",
    "#         try:\n",
    "#             while True:\n",
    "#                 msg = self.consumer.poll(timeout=1.0)\n",
    "#                 if msg is None:\n",
    "#                     continue\n",
    "#                 if msg.error():\n",
    "#                     raise Exception(msg.error())\n",
    "#                 else:\n",
    "#                     record = json.loads(msg.value().decode('utf-8'))\n",
    "#                     vehicle_year = int(record['Vehicle Year'])\n",
    "#                     self.total_samples += 1\n",
    "\n",
    "#                     if vehicle_year == 0:\n",
    "#                         if self.warmup_complete:\n",
    "#                             features = self.extract_features(record)\n",
    "#                             inferred_year = self.infer_vehicle_year(features)\n",
    "#                             record['Vehicle Year'] = inferred_year\n",
    "#                             # self.producer.produce('inferred_vehicle_years', key=str(record['Summons Number']), value=json.dumps(record))\n",
    "#                             print(f\"Inferred vehicle year {inferred_year} for ticket {record['Summons Number']}\")\n",
    "#                     else:\n",
    "#                         self.collect_samples(record)\n",
    "#                         if self.sample_count[vehicle_year] > 1000:\n",
    "#                             self.update_centroid(vehicle_year, self.extract_features(record))\n",
    "\n",
    "#                     if self.total_samples % 3000 == 0:\n",
    "#                         self.evaluate_model()\n",
    "#         except KeyboardInterrupt:\n",
    "#             print(\"Consumer interrupted.\")\n",
    "#         finally:\n",
    "#             self.consumer.close()\n",
    "#             print(\"Finished receiving all data\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     consumer = VehicleYearInferenceConsumer(\n",
    "#         topic='NYTickets',\n",
    "#         bootstrap_servers='localhost:9091,localhost:9092,localhost:9093',\n",
    "#         group_id='vehicle_year_inference'\n",
    "#     )\n",
    "#     consumer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting consumer_knn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile consumer_knn.py\n",
    "import confluent_kafka as kafka\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class VehicleYearInferenceConsumer:\n",
    "    def __init__(self, topic, bootstrap_servers, group_id):\n",
    "        self.consumer = kafka.Consumer({\n",
    "            'bootstrap.servers': bootstrap_servers,\n",
    "            'group.id': group_id,\n",
    "            'auto.offset.reset': 'earliest'\n",
    "        })\n",
    "        # self.producer = kafka.Producer({\n",
    "        #     'bootstrap.servers': bootstrap_servers\n",
    "        # })\n",
    "        self.topic = topic\n",
    "        self.consumer.subscribe([topic])\n",
    "        self.samples = defaultdict(list)\n",
    "        self.centroids = {}\n",
    "        self.sample_count = defaultdict(int)\n",
    "        self.total_samples = 0\n",
    "        self.warmup_complete = False\n",
    "\n",
    "        # # zero missing values columns\n",
    "        # self.columns = ['Registration State', 'Plate Type', 'Violation Code',\n",
    "        #             'Issuing Agency', 'Street Code1', 'Street Code2', 'Street Code3',\n",
    "        #             'Vehicle Expiration Date', 'Violation Precinct', 'Issuer Precinct',\n",
    "        #             'Issuer Code', 'Violation Time', 'Date First Observed', 'Law Section',\n",
    "        #             'Vehicle Year', 'Feet From Curb']\n",
    "        # # numeric columns\n",
    "        # self.columns = ['Violation Code', 'Street Code1', 'Street Code2',\n",
    "        #                 'Street Code3', 'Vehicle Expiration Date', 'Violation Location',\n",
    "        #                 'Violation Precinct', 'Issuer Precinct', 'Issuer Code',\n",
    "        #                 'Date First Observed', 'Law Section', 'Unregistered Vehicle?',\n",
    "        #                 'Vehicle Year', 'Feet From Curb']\n",
    "\n",
    "        self.categorical_columns = ['Registration State', 'Plate Type', 'Vehicle Body Type', 'Vehicle Make', \n",
    "                                    'Issuing Agency', 'Vehicle Color']\n",
    "        self.numeric_columns = ['Violation Code', 'Street Code1', 'Street Code2', 'Street Code3', \n",
    "                                'Violation Precinct', 'Issuer Precinct', 'Issuer Code', \n",
    "                                'Feet From Curb', 'Violation Location', 'Unregistered Vehicle?']\n",
    "        self.date_columns = ['Issue Date', 'Vehicle Expiration Date', 'Date First Observed']\n",
    "        self.time_columns = ['Violation Time']\n",
    "\n",
    "        self.onehot_encoders = {col: OneHotEncoder(handle_unknown='ignore') for col in self.categorical_columns}\n",
    "        self.prepare_encoders()\n",
    "\n",
    "    def prepare_encoders(self):\n",
    "        # Load unique values from JSON file\n",
    "        with open('unique_values.json', 'r') as f:\n",
    "            unique_values = json.load(f)\n",
    "        \n",
    "        # Replace NaN (which is a float) with \"Unknown\"\n",
    "        for key, values in unique_values.items():\n",
    "            unique_values[key] = [\"Unknown\" if isinstance(v, float) and np.isnan(v) else v for v in values]\n",
    "        \n",
    "        # Fit one-hot encoders with the unique values\n",
    "        for column, encoder in self.onehot_encoders.items():\n",
    "            unique_value_array = np.array(unique_values[column]).reshape(-1, 1)\n",
    "            encoder.fit(unique_value_array)\n",
    "\n",
    "    def extract_features(self, record):\n",
    "        features = []\n",
    "        \n",
    "        # Handle numeric columns\n",
    "        for column in self.numeric_columns:\n",
    "            value = record.get(column, 0)\n",
    "            try:\n",
    "                features.append(float(value) if value else 0.0)\n",
    "            except ValueError:\n",
    "                features.append(0.0)\n",
    "        \n",
    "        # Handle categorical columns with one-hot encoding\n",
    "        for column in self.categorical_columns:\n",
    "            value = record.get(column, 'Unknown')\n",
    "            encoded = self.onehot_encoders[column].transform([[value]]).toarray().flatten()\n",
    "            features.extend(encoded.tolist())\n",
    "        \n",
    "        # Handle date columns\n",
    "        for column in self.date_columns:\n",
    "            date_str = record.get(column, '01/01/1970')\n",
    "            try:\n",
    "                if '/' not in date_str:\n",
    "                    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "                else:\n",
    "                    date_obj = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "            except ValueError:\n",
    "                # print(f\"Invalid date format {date_str}\")\n",
    "                date_obj = datetime.strptime('01/01/1970', '%m/%d/%Y')\n",
    "            features.extend([date_obj.year, date_obj.month, date_obj.day])\n",
    "        \n",
    "        # Handle time columns\n",
    "        for column in self.time_columns:\n",
    "            time_str = record.get(column, '0000A')\n",
    "            minutes = self.convert_to_minutes(time_str)\n",
    "            features.append(minutes)\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "    def convert_to_minutes(self, time_str):\n",
    "        if time_str[-1] == 'P' and int(time_str[:2]) != 12:\n",
    "            minutes = (int(time_str[:2]) + 12) * 60 + int(time_str[2:4])\n",
    "        elif time_str[-1] == 'A' and int(time_str[:2]) == 12:\n",
    "            minutes = int(time_str[2:4])\n",
    "        else:\n",
    "            minutes = int(time_str[:2]) * 60 + int(time_str[2:4])\n",
    "        return minutes\n",
    "    \n",
    "    def collect_samples(self, record):\n",
    "        vehicle_year = int(record['Vehicle Year'])\n",
    "        if vehicle_year != 0:\n",
    "            features = self.extract_features(record)\n",
    "            self.samples[vehicle_year].append(features)\n",
    "            self.sample_count[vehicle_year] += 1\n",
    "            if self.sample_count[vehicle_year] == 1000 and vehicle_year not in self.centroids:\n",
    "                self.calculate_centroid(vehicle_year)\n",
    "\n",
    "    def calculate_centroid(self, vehicle_year):\n",
    "        self.centroids[vehicle_year] = np.mean(self.samples[vehicle_year], axis=0)\n",
    "        self.samples[vehicle_year] = []\n",
    "        if len(self.centroids) >= 10:\n",
    "            self.warmup_complete = True\n",
    "        print(f\"Calculated centroid for vehicle year {vehicle_year}\")\n",
    "\n",
    "    def update_centroid(self, vehicle_year):\n",
    "        if vehicle_year in self.centroids:\n",
    "            current_centroid = self.centroids[vehicle_year]\n",
    "            total_count = self.sample_count[vehicle_year]\n",
    "            new_samples = np.array(self.samples[vehicle_year])\n",
    "            new_count = len(new_samples)\n",
    "            current_count = total_count - new_count\n",
    "            \n",
    "            updated_centroid = (current_centroid * current_count + new_samples.mean(axis=0) * new_count) / (current_count + new_count)\n",
    "            \n",
    "            self.centroids[vehicle_year] = updated_centroid\n",
    "            # self.sample_count[vehicle_year] += new_count\n",
    "            self.samples[vehicle_year] = []\n",
    "            print(f\"Updated centroid for vehicle year {vehicle_year}\")\n",
    "\n",
    "    def infer_vehicle_year(self, features):\n",
    "        distances = {year: np.linalg.norm(features - centroid) for year, centroid in self.centroids.items()}\n",
    "        return min(distances, key=distances.get)\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        known_years = []\n",
    "        inferred_years = []\n",
    "        for year, samples in self.samples.items():\n",
    "            for sample in samples[:100]:\n",
    "                inferred_year = self.infer_vehicle_year(sample)\n",
    "                known_years.append(year)\n",
    "                inferred_years.append(inferred_year)\n",
    "        rmse = mean_squared_error(known_years, inferred_years, squared=False)\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        abs_avg_error = np.mean(np.abs(np.array(known_years) - np.array(inferred_years)))\n",
    "        print(f\"Mean Absolute Error: {abs_avg_error}\")\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            while True:\n",
    "                msg = self.consumer.poll(timeout=1.0)\n",
    "                if msg is None:\n",
    "                    continue\n",
    "                if msg.error():\n",
    "                    raise Exception(msg.error())\n",
    "                else:\n",
    "                    record = json.loads(msg.value().decode('utf-8'))\n",
    "                    try:\n",
    "                        vehicle_year = int(record['Vehicle Year'])\n",
    "                    except ValueError:\n",
    "                        vehicle_year = 0\n",
    "                        print(f\"Invalid vehicle year {record['Vehicle Year']} for ticket {record['Summons Number']}\")\n",
    "                    self.total_samples += 1\n",
    "\n",
    "                    if vehicle_year == 0:\n",
    "                        if self.warmup_complete:\n",
    "                            features = self.extract_features(record)\n",
    "                            inferred_year = self.infer_vehicle_year(features)\n",
    "                            record['Vehicle Year'] = inferred_year\n",
    "                            # self.producer.produce('inferred_vehicle_years', key=str(record['Summons Number']), value=json.dumps(record))\n",
    "                            print(f\"Inferred vehicle year {inferred_year} for ticket {record['Summons Number']}\")\n",
    "                    else:\n",
    "                        self.collect_samples(record)\n",
    "                        # print(self.sample_count)\n",
    "                        if (self.sample_count[vehicle_year]+1) % 1000 == 0 and vehicle_year in self.centroids:\n",
    "                            self.update_centroid(vehicle_year)\n",
    "\n",
    "                    if self.total_samples % 3000 == 0 and self.warmup_complete:\n",
    "                        self.evaluate_model()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Consumer interrupted.\")\n",
    "        finally:\n",
    "            self.consumer.close()\n",
    "            print(\"Finished receiving all data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consumer = VehicleYearInferenceConsumer(\n",
    "        topic='NYTickets',\n",
    "        bootstrap_servers='localhost:9091,localhost:9092,localhost:9093',\n",
    "        group_id='vehicle_year_inference'\n",
    "    )\n",
    "    consumer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting consumer_sgd.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile consumer_sgd.py\n",
    "# import confluent_kafka as kafka\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.linear_model import SGDRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from datetime import datetime\n",
    "\n",
    "# class VehicleYearInferenceWithSGD:\n",
    "#     def __init__(self, topic, bootstrap_servers, group_id):\n",
    "#         self.consumer = kafka.Consumer({\n",
    "#             'bootstrap.servers': bootstrap_servers,\n",
    "#             'group.id': group_id,\n",
    "#             'auto.offset.reset': 'earliest'\n",
    "#         })\n",
    "#         self.topic = topic\n",
    "#         self.consumer.subscribe([topic])\n",
    "        \n",
    "#         self.categorical_columns = ['Registration State', 'Plate Type', 'Vehicle Body Type', 'Vehicle Make', \n",
    "#                                     'Issuing Agency', 'Vehicle Color']\n",
    "#         self.numeric_columns = ['Violation Code', 'Street Code1', 'Street Code2', 'Street Code3', \n",
    "#                                 'Violation Precinct', 'Issuer Precinct', 'Issuer Code', \n",
    "#                                 'Feet From Curb', 'Violation Location', 'Unregistered Vehicle?']\n",
    "#         self.date_columns = ['Issue Date', 'Vehicle Expiration Date', 'Date First Observed']\n",
    "#         self.time_columns = ['Violation Time']\n",
    "\n",
    "#         self.onehot_encoders = {col: OneHotEncoder(handle_unknown='ignore') for col in self.categorical_columns}\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.model = SGDRegressor()\n",
    "#         self.total_samples = 0\n",
    "#         self.eval_samples_collected = 0\n",
    "#         self.eval_features = []\n",
    "#         self.eval_labels = []\n",
    "#         self.mean_shift = 2000\n",
    "#         self.prepare_encoders()\n",
    "\n",
    "#     def prepare_encoders(self):\n",
    "#         with open('unique_values.json', 'r') as f:\n",
    "#             unique_values = json.load(f)\n",
    "        \n",
    "#         for key, values in unique_values.items():\n",
    "#             unique_values[key] = [\"Unknown\" if isinstance(v, float) and np.isnan(v) else v for v in values]\n",
    "        \n",
    "#         for column, encoder in self.onehot_encoders.items():\n",
    "#             unique_value_array = np.array(unique_values[column]).reshape(-1, 1)\n",
    "#             encoder.fit(unique_value_array)\n",
    "\n",
    "#     def extract_features(self, record):\n",
    "#         features = []\n",
    "\n",
    "#         for column in self.numeric_columns:\n",
    "#             value = record.get(column, 0)\n",
    "#             try:\n",
    "#                 features.append(float(value) if value else 0.0)\n",
    "#             except ValueError:\n",
    "#                 features.append(0.0)\n",
    "        \n",
    "#         for column in self.categorical_columns:\n",
    "#             value = record.get(column, 'Unknown')\n",
    "#             encoded = self.onehot_encoders[column].transform([[value]]).toarray().flatten()\n",
    "#             features.extend(encoded.tolist())\n",
    "        \n",
    "#         for column in self.date_columns:\n",
    "#             date_str = record.get(column, '01/01/1970')\n",
    "#             try:\n",
    "#                 if '/' not in date_str:\n",
    "#                     date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "#                 else:\n",
    "#                     date_obj = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "#             except ValueError:\n",
    "#                 date_obj = datetime.strptime('01/01/1970', '%m/%d/%Y')\n",
    "#             features.extend([date_obj.year, date_obj.month, date_obj.day])\n",
    "        \n",
    "#         for column in self.time_columns:\n",
    "#             time_str = record.get(column, '0000A')\n",
    "#             minutes = self.convert_to_minutes(time_str)\n",
    "#             features.append(minutes)\n",
    "        \n",
    "#         return np.array(features)\n",
    "\n",
    "#     def convert_to_minutes(self, time_str):\n",
    "#         if time_str[-1] == 'P' and int(time_str[:2]) != 12:\n",
    "#             minutes = (int(time_str[:2]) + 12) * 60 + int(time_str[2:4])\n",
    "#         elif time_str[-1] == 'A' and int(time_str[:2]) == 12:\n",
    "#             minutes = int(time_str[2:4])\n",
    "#         else:\n",
    "#             minutes = int(time_str[:2]) * 60 + int(time_str[2:4])\n",
    "#         return minutes\n",
    "    \n",
    "#     def collect_samples(self, record):\n",
    "#         vehicle_year = int(record['Vehicle Year'])\n",
    "#         features = self.extract_features(record)\n",
    "#         if vehicle_year != 0:\n",
    "#             self.model.partial_fit([features], [vehicle_year-self.mean_shift])\n",
    "\n",
    "#     def infer_vehicle_year(self, features):\n",
    "#         return self.model.predict([features])[0] + self.mean_shift\n",
    "\n",
    "#     def evaluate_model(self):\n",
    "#         known_years = []\n",
    "#         inferred_years = []\n",
    "#         for features, actual_year in zip(self.eval_features, self.eval_labels):\n",
    "#             inferred_year = self.infer_vehicle_year(features)\n",
    "#             known_years.append(actual_year)\n",
    "#             inferred_years.append(inferred_year)\n",
    "#         rmse = mean_squared_error(known_years, inferred_years, squared=False)\n",
    "#         print(f\"RMSE: {rmse}\")\n",
    "#         abs_avg_error = np.mean(np.abs(np.array(known_years) - np.array(inferred_years)))\n",
    "#         print(f\"Mean Absolute Error: {abs_avg_error}\")\n",
    "#         self.eval_features = []\n",
    "#         self.eval_labels = []\n",
    "\n",
    "#     def run(self):\n",
    "#         try:\n",
    "#             while True:\n",
    "#                 msg = self.consumer.poll(timeout=1.0)\n",
    "#                 if msg is None:\n",
    "#                     continue\n",
    "#                 if msg.error():\n",
    "#                     raise Exception(msg.error())\n",
    "#                 else:\n",
    "#                     record = json.loads(msg.value().decode('utf-8'))\n",
    "#                     try:\n",
    "#                         vehicle_year = int(record['Vehicle Year'])\n",
    "#                     except ValueError:\n",
    "#                         vehicle_year = 0\n",
    "#                         print(f\"Invalid vehicle year {record['Vehicle Year']} for ticket {record['Summons Number']}\")\n",
    "#                     self.total_samples += 1\n",
    "\n",
    "#                     if self.total_samples % 3000 < 100:\n",
    "#                         if vehicle_year != 0:\n",
    "#                             features = self.extract_features(record)\n",
    "#                             self.eval_features.append(features)\n",
    "#                             self.eval_labels.append(vehicle_year)\n",
    "#                             self.eval_samples_collected += 1\n",
    "#                             if self.eval_samples_collected == 100:\n",
    "#                                 self.evaluate_model()\n",
    "#                                 self.eval_samples_collected = 0\n",
    "#                     else:\n",
    "#                         if vehicle_year == 0 and self.total_samples > 1000:\n",
    "#                             features = self.extract_features(record)\n",
    "#                             inferred_year = self.infer_vehicle_year(features)\n",
    "#                             record['Vehicle Year'] = inferred_year\n",
    "#                             print(f\"Inferred vehicle year {inferred_year} for ticket {record['Summons Number']}\")\n",
    "#                         else:\n",
    "#                             self.collect_samples(record)\n",
    "\n",
    "#         except KeyboardInterrupt:\n",
    "#             print(\"Consumer interrupted.\")\n",
    "#         finally:\n",
    "#             self.consumer.close()\n",
    "#             print(\"Finished receiving all data\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     consumer = VehicleYearInferenceWithSGD(\n",
    "#         topic='NYTickets',\n",
    "#         bootstrap_servers='localhost:9091,localhost:9092,localhost:9093',\n",
    "#         group_id='vehicle_year_inference'\n",
    "#     )\n",
    "#     consumer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting consumer_sgd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile consumer_sgd.py\n",
    "import confluent_kafka as kafka\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "\n",
    "class VehicleYearInferenceWithSGD:\n",
    "    def __init__(self, topic, bootstrap_servers, group_id):\n",
    "        self.consumer = kafka.Consumer({\n",
    "            'bootstrap.servers': bootstrap_servers,\n",
    "            'group.id': group_id,\n",
    "            'auto.offset.reset': 'earliest'\n",
    "        })\n",
    "        self.topic = topic\n",
    "        self.consumer.subscribe([topic])\n",
    "        \n",
    "        self.categorical_columns = ['Registration State', 'Plate Type', 'Vehicle Body Type', 'Vehicle Make', \n",
    "                                    'Issuing Agency', 'Vehicle Color']\n",
    "        self.numeric_columns = ['Violation Code', 'Street Code1', 'Street Code2', 'Street Code3', \n",
    "                                'Violation Precinct', 'Issuer Precinct', 'Issuer Code', \n",
    "                                'Feet From Curb', 'Violation Location', 'Unregistered Vehicle?']\n",
    "        self.date_columns = ['Issue Date', 'Vehicle Expiration Date', 'Date First Observed']\n",
    "        self.time_columns = ['Violation Time']\n",
    "\n",
    "        self.onehot_encoders = {col: OneHotEncoder(handle_unknown='ignore') for col in self.categorical_columns}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = SGDRegressor()\n",
    "        self.total_samples = 0\n",
    "        self.eval_samples_collected = 0\n",
    "        self.eval_features = []\n",
    "        self.eval_labels = []\n",
    "        self.warmup_samples = []\n",
    "        self.warmup_labels = []\n",
    "        self.mean_shift = 2000\n",
    "        self.prepare_encoders()\n",
    "        self.warmup_complete = False\n",
    "\n",
    "    def prepare_encoders(self):\n",
    "        with open('unique_values.json', 'r') as f:\n",
    "            unique_values = json.load(f)\n",
    "        \n",
    "        for key, values in unique_values.items():\n",
    "            unique_values[key] = [\"Unknown\" if isinstance(v, float) and np.isnan(v) else v for v in values]\n",
    "        \n",
    "        for column, encoder in self.onehot_encoders.items():\n",
    "            unique_value_array = np.array(unique_values[column]).reshape(-1, 1)\n",
    "            encoder.fit(unique_value_array)\n",
    "\n",
    "    def extract_features(self, record):\n",
    "        features = []\n",
    "\n",
    "        for column in self.numeric_columns:\n",
    "            value = record.get(column, 0)\n",
    "            try:\n",
    "                features.append(float(value) if value else 0.0)\n",
    "            except ValueError:\n",
    "                features.append(0.0)\n",
    "        \n",
    "        for column in self.categorical_columns:\n",
    "            value = record.get(column, 'Unknown')\n",
    "            encoded = self.onehot_encoders[column].transform([[value]]).toarray().flatten()\n",
    "            features.extend(encoded.tolist())\n",
    "        \n",
    "        for column in self.date_columns:\n",
    "            date_str = record.get(column, '01/01/1970')\n",
    "            try:\n",
    "                if '/' not in date_str:\n",
    "                    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "                else:\n",
    "                    date_obj = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "            except ValueError:\n",
    "                date_obj = datetime.strptime('01/01/1970', '%m/%d/%Y')\n",
    "            features.extend([date_obj.year, date_obj.month, date_obj.day])\n",
    "        \n",
    "        for column in self.time_columns:\n",
    "            time_str = record.get(column, '0000A')\n",
    "            minutes = self.convert_to_minutes(time_str)\n",
    "            features.append(minutes)\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "    def convert_to_minutes(self, time_str):\n",
    "        if time_str[-1] == 'P' and int(time_str[:2]) != 12:\n",
    "            minutes = (int(time_str[:2]) + 12) * 60 + int(time_str[2:4])\n",
    "        elif time_str[-1] == 'A' and int(time_str[:2]) == 12:\n",
    "            minutes = int(time_str[2:4])\n",
    "        else:\n",
    "            minutes = int(time_str[:2]) * 60 + int(time_str[2:4])\n",
    "        return minutes\n",
    "\n",
    "    def warmup_phase(self, record):\n",
    "        vehicle_year = int(record['Vehicle Year'])\n",
    "        features = self.extract_features(record)\n",
    "        if vehicle_year != 0:\n",
    "            self.warmup_samples.append(features)\n",
    "            self.warmup_labels.append(vehicle_year - self.mean_shift)\n",
    "            if len(self.warmup_samples) >= 1000:\n",
    "                self.scaler.fit(self.warmup_samples)\n",
    "                scaled_samples = self.scaler.transform(self.warmup_samples)\n",
    "                self.model.partial_fit(scaled_samples, self.warmup_labels)\n",
    "                self.warmup_complete = True\n",
    "                self.warmup_samples = []\n",
    "                self.warmup_labels = []\n",
    "\n",
    "    def collect_samples(self, record):\n",
    "        vehicle_year = int(record['Vehicle Year'])\n",
    "        features = self.extract_features(record)\n",
    "        if vehicle_year != 0:\n",
    "            scaled_features = self.scaler.transform([features])\n",
    "            self.model.partial_fit(scaled_features, [vehicle_year - self.mean_shift])\n",
    "\n",
    "    def infer_vehicle_year(self, features):\n",
    "        scaled_features = self.scaler.transform([features])\n",
    "        return self.model.predict(scaled_features)[0] + self.mean_shift\n",
    "        # return 2013\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        known_years = []\n",
    "        inferred_years = []\n",
    "        for features, actual_year in zip(self.eval_features, self.eval_labels):\n",
    "            inferred_year = self.infer_vehicle_year(features)\n",
    "            known_years.append(actual_year)\n",
    "            inferred_years.append(inferred_year)\n",
    "        rmse = mean_squared_error(known_years, inferred_years, squared=False)\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        abs_avg_error = np.mean(np.abs(np.array(known_years) - np.array(inferred_years)))\n",
    "        print(f\"Mean Absolute Error: {abs_avg_error}\")\n",
    "        self.eval_features = []\n",
    "        self.eval_labels = []\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            while True:\n",
    "                msg = self.consumer.poll(timeout=1.0)\n",
    "                if msg is None:\n",
    "                    continue\n",
    "                if msg.error():\n",
    "                    raise Exception(msg.error())\n",
    "                else:\n",
    "                    record = json.loads(msg.value().decode('utf-8'))\n",
    "                    try:\n",
    "                        vehicle_year = int(record['Vehicle Year'])\n",
    "                    except ValueError:\n",
    "                        vehicle_year = 0\n",
    "                        print(f\"Invalid vehicle year {record['Vehicle Year']} for ticket {record['Summons Number']}\")\n",
    "                    self.total_samples += 1\n",
    "\n",
    "                    if not self.warmup_complete:\n",
    "                        self.warmup_phase(record)\n",
    "                    else:\n",
    "                        if self.total_samples % 3000 < 100:\n",
    "                            if vehicle_year != 0:\n",
    "                                features = self.extract_features(record)\n",
    "                                self.eval_features.append(features)\n",
    "                                self.eval_labels.append(vehicle_year)\n",
    "                                self.eval_samples_collected += 1\n",
    "                                if self.eval_samples_collected == 100:\n",
    "                                    self.evaluate_model()\n",
    "                                    self.eval_samples_collected = 0\n",
    "                        else:\n",
    "                            if vehicle_year == 0:\n",
    "                                features = self.extract_features(record)\n",
    "                                inferred_year = self.infer_vehicle_year(features)\n",
    "                                inferred_year = int(inferred_year)\n",
    "                                record['Vehicle Year'] = inferred_year\n",
    "                                print(f\"Inferred vehicle year {inferred_year} for ticket {record['Summons Number']}\")\n",
    "                            else:\n",
    "                                self.collect_samples(record)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Consumer interrupted.\")\n",
    "        finally:\n",
    "            self.consumer.close()\n",
    "            print(\"Finished receiving all data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consumer = VehicleYearInferenceWithSGD(\n",
    "        topic='NYTickets',\n",
    "        bootstrap_servers='localhost:9091,localhost:9092,localhost:9093',\n",
    "        group_id='vehicle_year_inference'\n",
    "    )\n",
    "    consumer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting consumer_ipca_knn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile consumer_ipca_knn.py\n",
    "import confluent_kafka as kafka\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "\n",
    "class VehicleYearInferenceWithIPCAKNN:\n",
    "    def __init__(self, topic, bootstrap_servers, group_id):\n",
    "        self.consumer = kafka.Consumer({\n",
    "            'bootstrap.servers': bootstrap_servers,\n",
    "            'group.id': group_id,\n",
    "            'auto.offset.reset': 'earliest'\n",
    "        })\n",
    "        self.topic = topic\n",
    "        self.consumer.subscribe([topic])\n",
    "        \n",
    "        self.categorical_columns = ['Registration State', 'Plate Type', 'Vehicle Body Type', 'Vehicle Make', \n",
    "                                    'Issuing Agency', 'Vehicle Color']\n",
    "        self.numeric_columns = ['Violation Code', 'Street Code1', 'Street Code2', 'Street Code3', \n",
    "                                'Violation Precinct', 'Issuer Precinct', 'Issuer Code', \n",
    "                                'Feet From Curb', 'Violation Location', 'Unregistered Vehicle?']\n",
    "        self.date_columns = ['Issue Date', 'Vehicle Expiration Date', 'Date First Observed']\n",
    "        self.time_columns = ['Violation Time']\n",
    "\n",
    "        self.onehot_encoders = {col: OneHotEncoder(handle_unknown='ignore') for col in self.categorical_columns}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.ipca = IncrementalPCA(n_components=10)\n",
    "        self.knn = KNeighborsRegressor(n_neighbors=5)\n",
    "        self.buffer = []\n",
    "        self.total_samples = 0\n",
    "        self.mean_shift = 2000\n",
    "        self.prepare_encoders()\n",
    "        self.already_fit = False\n",
    "\n",
    "    def prepare_encoders(self):\n",
    "        with open('unique_values.json', 'r') as f:\n",
    "            unique_values = json.load(f)\n",
    "        \n",
    "        for key, values in unique_values.items():\n",
    "            unique_values[key] = [\"Unknown\" if isinstance(v, float) and np.isnan(v) else v for v in values]\n",
    "        \n",
    "        for column, encoder in self.onehot_encoders.items():\n",
    "            unique_value_array = np.array(unique_values[column]).reshape(-1, 1)\n",
    "            encoder.fit(unique_value_array)\n",
    "\n",
    "    def extract_features(self, record):\n",
    "        features = []\n",
    "\n",
    "        for column in self.numeric_columns:\n",
    "            value = record.get(column, 0)\n",
    "            try:\n",
    "                features.append(float(value) if value else 0.0)\n",
    "            except ValueError:\n",
    "                features.append(0.0)\n",
    "        \n",
    "        for column in self.categorical_columns:\n",
    "            value = record.get(column, 'Unknown')\n",
    "            encoded = self.onehot_encoders[column].transform([[value]]).toarray().flatten()\n",
    "            features.extend(encoded.tolist())\n",
    "        \n",
    "        for column in self.date_columns:\n",
    "            date_str = record.get(column, '01/01/1970')\n",
    "            try:\n",
    "                if '/' not in date_str:\n",
    "                    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "                else:\n",
    "                    date_obj = datetime.strptime(date_str, '%m/%d/%Y')\n",
    "            except ValueError:\n",
    "                date_obj = datetime.strptime('01/01/1970', '%m/%d/%Y')\n",
    "            features.extend([date_obj.year, date_obj.month, date_obj.day])\n",
    "        \n",
    "        for column in self.time_columns:\n",
    "            time_str = record.get(column, '0000A')\n",
    "            minutes = self.convert_to_minutes(time_str)\n",
    "            features.append(minutes)\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "    def convert_to_minutes(self, time_str):\n",
    "        if time_str[-1] == 'P' and int(time_str[:2]) != 12:\n",
    "            minutes = (int(time_str[:2]) + 12) * 60 + int(time_str[2:4])\n",
    "        elif time_str[-1] == 'A' and int(time_str[:2]) == 12:\n",
    "            minutes = int(time_str[2:4])\n",
    "        else:\n",
    "            minutes = int(time_str[:2]) * 60 + int(time_str[2:4])\n",
    "        return minutes\n",
    "\n",
    "    def update_models(self):\n",
    "        buffer_features, buffer_labels = zip(*self.buffer)\n",
    "        buffer_features = np.array(buffer_features)\n",
    "        buffer_labels = np.array(buffer_labels)\n",
    "\n",
    "        # Fit scaler and IPCA incrementally\n",
    "        self.scaler.partial_fit(buffer_features)\n",
    "        scaled_features = self.scaler.transform(buffer_features)\n",
    "        self.ipca.partial_fit(scaled_features)\n",
    "\n",
    "        # Transform features using updated IPCA\n",
    "        transformed_features = self.ipca.transform(scaled_features)\n",
    "\n",
    "        # Update kNN model\n",
    "        self.knn.fit(transformed_features, buffer_labels)\n",
    "\n",
    "        self.buffer = []\n",
    "\n",
    "    def infer_vehicle_year(self, features):\n",
    "        scaled_features = self.scaler.transform([features])\n",
    "        transformed_features = self.ipca.transform(scaled_features)\n",
    "        return self.knn.predict(transformed_features)[0] + self.mean_shift\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        known_years = []\n",
    "        inferred_years = []\n",
    "        for features, actual_year in self.buffer:\n",
    "            inferred_year = self.infer_vehicle_year(features)\n",
    "            known_years.append(actual_year + self.mean_shift)\n",
    "            inferred_years.append(inferred_year)\n",
    "        rmse = mean_squared_error(known_years, inferred_years, squared=False)\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        abs_avg_error = np.mean(np.abs(np.array(known_years) - np.array(inferred_years)))\n",
    "        print(f\"Mean Absolute Error: {abs_avg_error}\")\n",
    "        self.buffer = []\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            while True:\n",
    "                msg = self.consumer.poll(timeout=1.0)\n",
    "                if msg is None:\n",
    "                    continue\n",
    "                if msg.error():\n",
    "                    raise Exception(msg.error())\n",
    "                record = json.loads(msg.value().decode('utf-8'))\n",
    "                try:\n",
    "                    vehicle_year = int(record['Vehicle Year'])\n",
    "                except ValueError:\n",
    "                    vehicle_year = 0\n",
    "                    print(f\"Invalid vehicle year {record['Vehicle Year']} for ticket {record['Summons Number']}\")\n",
    "                self.total_samples += 1\n",
    "\n",
    "                if vehicle_year == 0:\n",
    "                    if self.already_fit:\n",
    "                        features = self.extract_features(record)\n",
    "                        inferred_year = self.infer_vehicle_year(features)\n",
    "                        record['Vehicle Year'] = int(inferred_year)\n",
    "                        print(f\"Inferred vehicle year {record['Vehicle Year']} for ticket {record['Summons Number']}\")\n",
    "                else:\n",
    "                    features = self.extract_features(record)\n",
    "                    self.buffer.append((features, vehicle_year - self.mean_shift))\n",
    "                    \n",
    "                    if len(self.buffer) >= 1000:\n",
    "                        self.update_models()\n",
    "                        self.already_fit = True\n",
    "                    \n",
    "                    if len(self.buffer) >= 100 and self.total_samples % 3000 < 100:\n",
    "                        self.evaluate_model()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Consumer interrupted.\")\n",
    "        finally:\n",
    "            self.consumer.close()\n",
    "            print(\"Finished receiving all data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consumer = VehicleYearInferenceWithIPCAKNN(\n",
    "        topic='NYTickets',\n",
    "        bootstrap_servers='localhost:9091,localhost:9092,localhost:9093',\n",
    "        group_id='vehicle_year_inference'\n",
    "    )\n",
    "    consumer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run producer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run consumer_all.py\n",
    "%run consumer_borough.py\n",
    "%run consumer_street.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
