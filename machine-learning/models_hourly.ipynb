{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DASK_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.wrappers import Incremental\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import LocalCluster\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import time\n",
    "from dask.delayed import delayed\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:39497' processes=64 threads=128, memory=119.21 GiB>\n"
     ]
    }
   ],
   "source": [
    "# Initialize Dask client\n",
    "cluster = LocalCluster(n_workers=64, threads_per_worker=2, memory_limit=\"2GB\")\n",
    "client = Client(cluster)\n",
    "# print client information\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summons Number                                 int64\n",
      "Plate ID                             string[pyarrow]\n",
      "Registration State                   string[pyarrow]\n",
      "Plate Type                           string[pyarrow]\n",
      "Issue Date                           string[pyarrow]\n",
      "Violation Code                                 int64\n",
      "Vehicle Body Type                    string[pyarrow]\n",
      "Vehicle Make                         string[pyarrow]\n",
      "Issuing Agency                       string[pyarrow]\n",
      "Street Code1                                   int64\n",
      "Street Code2                                   int64\n",
      "Street Code3                                   int64\n",
      "Vehicle Expiration Date              string[pyarrow]\n",
      "Violation Location                           float64\n",
      "Violation Precinct                             int64\n",
      "Issuer Precinct                                int64\n",
      "Issuer Code                                    int64\n",
      "Issuer Command                       string[pyarrow]\n",
      "Issuer Squad                         string[pyarrow]\n",
      "Violation Time                       string[pyarrow]\n",
      "Time First Observed                  string[pyarrow]\n",
      "Violation County                     string[pyarrow]\n",
      "Violation In Front Of Or Opposite    string[pyarrow]\n",
      "Number                               string[pyarrow]\n",
      "Street                               string[pyarrow]\n",
      "Intersecting Street                  string[pyarrow]\n",
      "Date First Observed                  string[pyarrow]\n",
      "Law Section                          string[pyarrow]\n",
      "Sub Division                         string[pyarrow]\n",
      "Violation Legal Code                 string[pyarrow]\n",
      "Days Parking In Effect               string[pyarrow]\n",
      "From Hours In Effect                 string[pyarrow]\n",
      "To Hours In Effect                   string[pyarrow]\n",
      "Vehicle Color                        string[pyarrow]\n",
      "Unregistered Vehicle?                string[pyarrow]\n",
      "Vehicle Year                         string[pyarrow]\n",
      "Meter Number                         string[pyarrow]\n",
      "Feet From Curb                       string[pyarrow]\n",
      "Violation Post Code                  string[pyarrow]\n",
      "Violation Description                string[pyarrow]\n",
      "No Standing or Stopping Violation    string[pyarrow]\n",
      "Hydrant Violation                    string[pyarrow]\n",
      "Double Parking Violation             string[pyarrow]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "files_to_use = \"parquet\"\n",
    "# files_to_use = \"hdf5\" # does not work\n",
    "\n",
    "if files_to_use == \"parquet\":\n",
    "    start_time = time.time()\n",
    "    data = dd.read_parquet('/d/hpc/projects/FRI/bigdata/students/dp8949/parquet_data/*.parquet')\n",
    "    end_time = time.time()\n",
    "\n",
    "elif files_to_use == \"hdf5\":\n",
    "    start_time = time.time()\n",
    "    data = dd.read_hdf('/d/hpc/projects/FRI/bigdata/students/dp8949/hdf5_data/*.h5', '/*')\n",
    "    end_time = time.time()\n",
    "else:\n",
    "    print(\"No data files found\")\n",
    "    exit(1)\n",
    "\n",
    "with open('results_comparison.txt', 'a') as f:\n",
    "    f.write(\"-\"*50 + \"\\n\")\n",
    "    f.write(f\"Data loaded from {files_to_use} files\\n\")\n",
    "    f.write(f\"Data shape: {data.shape}\\n\")\n",
    "    f.write(\"-\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"{'Task':<35}{'Time':<10}\\n\")\n",
    "    f.write(f\"{'Data reading':<35}{(end_time - start_time):<10.2f}\\n\")\n",
    "    \n",
    "\n",
    "# print all columns and their data types\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data):\n",
    "    # keep only the columns we need\n",
    "    columns_to_keep = [\n",
    "        'Violation County',\n",
    "        'Issue Date',\n",
    "        'Violation Time',]\n",
    "\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # rename columns to remove spaces and make them lowercase\n",
    "    data = data.rename(columns={\n",
    "        'Violation County': 'violation_county',\n",
    "        'Issue Date': 'issue_date',\n",
    "        'Violation Time': 'violation_time'\n",
    "    })\n",
    "\n",
    "    # fix the county names\n",
    "    remap_county_dict = {\n",
    "        'K' : 'Brooklyn',\n",
    "        'Q' : 'Queens',\n",
    "        'NY': 'Manhattan',\n",
    "        'QN': 'Queens',\n",
    "        'BK': 'Brooklyn',\n",
    "        'R' : 'Staten Island',\n",
    "        'BX': 'Bronx',\n",
    "        'ST': 'Staten Island',\n",
    "        'MN': 'Manhattan',\n",
    "        'KINGS': 'Brooklyn',\n",
    "        'QNS': 'Queens',\n",
    "        'BRONX': 'Bronx'\n",
    "    }\n",
    "    data['violation_county'] = data['violation_county'].map(remap_county_dict, meta=('violation_county', 'category')).astype('category')\n",
    "\n",
    "    # convert the Issue Date to a datetime object\n",
    "    data['issue_date'] = dd.to_datetime(data['issue_date'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "    # Remove 'A' and 'P' from the end of the time, add ' AM' or ' PM' accordingly\n",
    "    data['violation_time'] = data['violation_time'].str.slice(stop=-1) + ' ' + data['violation_time'].str.slice(start=-1).replace({'A': 'AM', 'P': 'PM'})\n",
    "\n",
    "    # Convert the Violation Time to a datetime object\n",
    "    data['violation_time'] = dd.to_datetime(data['violation_time'], format='%I%M %p', errors='coerce')\n",
    "\n",
    "    # create a new column for the day of the week the violation was issued\n",
    "    data['violation_day_week'] = data['issue_date'].dt.dayofweek\n",
    "\n",
    "    # create a new column for the day of the month the violation was issued\n",
    "    data['violation_day_month'] = data['issue_date'].dt.day\n",
    "\n",
    "    # create a new column for the month the violation was issued\n",
    "    data['violation_month'] = data['issue_date'].dt.month\n",
    "\n",
    "    # create a new column for the year the violation was issued\n",
    "    data['violation_year'] = data['issue_date'].dt.year\n",
    "\n",
    "    # keep only rows with valid year (2013-2024)\n",
    "    data = data[(data['violation_year'] >= 2013) & (data['violation_year'] <= 2024)]\n",
    "\n",
    "    # keep only rows with valid month (1-12)\n",
    "    data = data[(data['violation_month'] >= 1) & (data['violation_month'] <= 12)]\n",
    "\n",
    "    # drop all rows with missing values\n",
    "    data = data.dropna()\n",
    "\n",
    "    # create a new column for the hour of the day the violation was issued\n",
    "    data['violation_hour'] = data['violation_time'].dt.hour.astype('int32')\n",
    "\n",
    "    # drop the Issue Date and Violation Time columns\n",
    "    data = data.drop(columns=['violation_time'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "start_time = time.time()\n",
    "data = data_cleaning(data)\n",
    "\n",
    "# save the time to results_comparison.txt\n",
    "with open('results_comparison.txt', 'a') as f:\n",
    "    f.write(f\"{'Data Cleaning':<35}{time.time() - start_time:<10.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation_county             category\n",
      "issue_date             datetime64[ns]\n",
      "violation_day_week              int32\n",
      "violation_day_month             int32\n",
      "violation_month                 int32\n",
      "violation_year                  int32\n",
      "violation_hour                  int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print type of columns\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick only one county (if you want to, otherwise set to \"all\")\n",
    "\n",
    "# county = 'Manhattan'\n",
    "# county = 'Queens'\n",
    "# county = 'Brooklyn'\n",
    "# county = 'Bronx'\n",
    "# county = 'Staten Island'\n",
    "county = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if county != 'all':\n",
    "    data = data[data['violation_county'] == county]\n",
    "    # drop the Violation County column\n",
    "    data = data.drop(columns=['violation_county'])\n",
    "else:\n",
    "    # drop the Violation County column\n",
    "    data = data.drop(columns=['violation_county'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dataset to an hourly level - count the number of violations per hour\n",
    "data = data.groupby(['violation_year', 'violation_month', 'violation_day_month', 'violation_day_week', 'violation_hour']).size().reset_index()\n",
    "data = data.rename(columns={0: 'violation_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new column of format ='%Y-%m-%d %H:%M:%S' for the datetime (for joining with weather data)\n",
    "data['datetime'] = data['violation_year'].astype(int).astype(str) + '-' + \\\n",
    "                          data['violation_month'].astype(int).astype(str).str.zfill(2) + '-' + \\\n",
    "                          data['violation_day_month'].astype(int).astype(str).str.zfill(2) + ' ' + \\\n",
    "                          data['violation_hour'].astype(int).astype(str).str.zfill(2) + ':00:00'\n",
    "# data['datetime'] = dd.to_datetime(data['datetime'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by month and count the number of violations\n",
    "# monthly_violations = data.groupby(data['datetime'].dt.to_period('M')).size()\n",
    "\n",
    "# Plot the monthly violations\n",
    "# monthly_violations.compute().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the data take only the rows that have the datetime between 2013-01-01 and 2015-03-31\n",
    "# data = data[(data['datetime'] >= '2013-01-01') & (data['datetime'] <= '2015-03-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   violation_year  violation_month  violation_day_month  violation_day_week  \\\n",
      "0          2013.0              1.0                  1.0                 1.0   \n",
      "1          2013.0              1.0                  1.0                 1.0   \n",
      "2          2013.0              1.0                  1.0                 1.0   \n",
      "3          2013.0              1.0                  1.0                 1.0   \n",
      "4          2013.0              1.0                  1.0                 1.0   \n",
      "\n",
      "   violation_hour  violation_count             datetime  \n",
      "0               0                1  2013-01-01 00:00:00  \n",
      "1               1                1  2013-01-01 01:00:00  \n",
      "2               2                2  2013-01-01 02:00:00  \n",
      "3               3                1  2013-01-01 03:00:00  \n",
      "4               4                2  2013-01-01 04:00:00  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path pattern to read all CSV files (adjust the path as needed)\n",
    "file_pattern = 'nycweather_pesjak/nyc_w_*.csv'\n",
    "\n",
    "# Specify the data types for each column\n",
    "dtypes = {\n",
    "    'datetime': 'str',  # 'datetime' will be parsed separately\n",
    "    'temp': 'float64',\n",
    "    'feelslike': 'float64',\n",
    "    'dew': 'float64',\n",
    "    'humidity': 'float64',\n",
    "    'precip': 'float64',\n",
    "    'precipprob': 'float64',\n",
    "    'preciptype': 'object',  # String data type\n",
    "    'snow': 'float64',\n",
    "    'snowdepth': 'float64',\n",
    "    'windgust': 'float64',\n",
    "    'windspeed': 'float64',\n",
    "    'winddir': 'float64',\n",
    "    'sealevelpressure': 'float64',\n",
    "    'cloudcover': 'float64',\n",
    "    'visibility': 'float64',\n",
    "    'solarradiation': 'float64',\n",
    "    'solarenergy': 'float64',\n",
    "    'uvindex': 'float64',\n",
    "    'severerisk': 'float64',\n",
    "    'conditions': 'object',  # String data type\n",
    "    'icon': 'object',  # String data type\n",
    "    'stations': 'object'  # String data type\n",
    "}\n",
    "\n",
    "# Read all CSV files into a single Dask DataFrame with specified dtypes\n",
    "weather_df = dd.read_csv(file_pattern, dtype=dtypes)\n",
    "\n",
    "# Convert the 'datetime' column to datetime type\n",
    "weather_df['datetime'] = dd.to_datetime(weather_df['datetime'], format='%Y-%m-%dT%H:%M:%S', errors='coerce')\n",
    "\n",
    "# Drop all string columns except 'datetime'\n",
    "columns_to_keep = ['datetime'] + [col for col in weather_df.columns if weather_df[col].dtype in ['float64', 'int64']]\n",
    "weather_df = weather_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_data(data, weather_df):\n",
    "    # Step 4: Merge the complete data with weather data\n",
    "    augmented_data = dd.merge(data, weather_df, on='datetime', how='left')\n",
    "    \n",
    "    # Step 5: drop missing data and severerisk and windgust columns\n",
    "    augmented_data = augmented_data.drop(columns=['severerisk', 'windgust'])\n",
    "    augmented_data = augmented_data.dropna()\n",
    "\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_hours(data):\n",
    "    # Step 1: Create a complete range of dates and hours directly with hourly frequency\n",
    "    min_date = data['datetime'].min().compute()\n",
    "    max_date = data['datetime'].max().compute()\n",
    "    print(f\"Taking data from {min_date} to {max_date}\")\n",
    "    all_date_hours = pd.date_range(start=min_date, end=max_date, freq='h').to_frame(index=False, name='datetime')\n",
    "\n",
    "    # Extract year, month, day_of_month, and day_of_week directly from the datetime\n",
    "    all_date_hours['violation_year'] = all_date_hours['datetime'].dt.year\n",
    "    all_date_hours['violation_month'] = all_date_hours['datetime'].dt.month\n",
    "    all_date_hours['violation_day_month'] = all_date_hours['datetime'].dt.day\n",
    "    all_date_hours['violation_day_week'] = all_date_hours['datetime'].dt.dayofweek\n",
    "    all_date_hours['violation_hour'] = all_date_hours['datetime'].dt.hour\n",
    "\n",
    "    # Step 2: Convert the comprehensive Pandas DataFrame to a Dask DataFrame\n",
    "    all_date_hours_dd = dd.from_pandas(all_date_hours, npartitions=10)\n",
    "\n",
    "    # Step 3: Merge with existing data using Dask's merge function\n",
    "    complete_data = dd.merge(all_date_hours_dd, data, on=['violation_year', 'violation_month', 'violation_day_month', 'violation_day_week', 'violation_hour'], how='outer')\n",
    "    # complete_data = dd.merge(all_date_hours_dd, data, on=['violation_year', 'violation_month', 'violation_day_month', 'violation_day_week', 'violation_hour', 'ounty'], how='outer')\n",
    "\n",
    "    # Fill missing values with 0\n",
    "    complete_data['violation_count'] = complete_data['violation_count'].fillna(0).astype(int)\n",
    "    \n",
    "    # drop datetime_y column and rename datetime_x to datetime\n",
    "    complete_data = complete_data.drop(columns=['datetime_y'])\n",
    "    complete_data = complete_data.rename(columns={'datetime_x': 'datetime'})\n",
    "    \n",
    "    return complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking data from 2013-01-01 00:00:00 to 2024-12-31 21:00:00\n",
      "datetime               datetime64[ns]\n",
      "violation_year                  int32\n",
      "violation_month                 int32\n",
      "violation_day_month             int32\n",
      "violation_day_week              int32\n",
      "violation_hour                  int32\n",
      "violation_count                 int64\n",
      "temp                          float64\n",
      "feelslike                     float64\n",
      "dew                           float64\n",
      "humidity                      float64\n",
      "precip                        float64\n",
      "precipprob                    float64\n",
      "snow                          float64\n",
      "snowdepth                     float64\n",
      "windspeed                     float64\n",
      "winddir                       float64\n",
      "sealevelpressure              float64\n",
      "cloudcover                    float64\n",
      "visibility                    float64\n",
      "solarradiation                float64\n",
      "solarenergy                   float64\n",
      "uvindex                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# fill in the missing hours\n",
    "start_time = time.time()\n",
    "data = fill_in_hours(data)\n",
    "# save the time to results_comparison.txt\n",
    "with open('results_comparison.txt', 'a') as f:\n",
    "    f.write(f\"{'Fill in missing hours':<35}{time.time() - start_time:<10.2f}\\n\")\n",
    "\n",
    "# add weather data\n",
    "start_time = time.time()\n",
    "data = add_weather_data(data, weather_df)\n",
    "# save the time to results_comparison.txt\n",
    "with open('results_comparison.txt', 'a') as f:\n",
    "    f.write(f\"{'Add weather data':<35}{time.time() - start_time:<10.2f}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# print the data types\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Temporal train-test split\n",
    "    split_date = '2022-12-31 23:00:00'\n",
    "    train_data = data[data['datetime'] < split_date]\n",
    "    test_data = data[data['datetime'] >= split_date]\n",
    "    \n",
    "    print(f\"Temporal train-test split done in {time.time() - start_time} seconds ({time.time() - start_time})\")\n",
    "    new_start_time = time.time()\n",
    "\n",
    "    # Combine dropping of columns\n",
    "    columns_to_drop = ['violation_count', 'datetime']\n",
    "    X_train = train_data.drop(columns=columns_to_drop)\n",
    "    y_train = train_data['violation_count']\n",
    "    X_test = test_data.drop(columns=columns_to_drop)\n",
    "    y_test = test_data['violation_count']\n",
    "    \n",
    "    print(f\"Dropping columns done in {time.time() - new_start_time} seconds ({time.time() - start_time})\")\n",
    "    new_start_time = time.time()\n",
    "\n",
    "    # Persist the training and test sets to avoid recomputation\n",
    "    X_train, y_train, X_test, y_test = client.persist([X_train, y_train, X_test, y_test])\n",
    "    \n",
    "    print(f\"Persisting data done in {time.time() - new_start_time} seconds ({time.time() - start_time})\")\n",
    "    new_start_time = time.time()\n",
    "\n",
    "    # Identify and drop constant columns in a single pass\n",
    "    constant_columns = [col for col in X_train.columns if X_train[col].nunique().compute() <= 1]\n",
    "    X_train = X_train.drop(columns=constant_columns)\n",
    "    X_test = X_test.drop(columns=constant_columns)\n",
    "    \n",
    "    print(f\"Dropping constant columns done in {time.time() - new_start_time} seconds ({time.time() - start_time})\")\n",
    "    new_start_time = time.time()\n",
    "    # print how many columns were dropped\n",
    "    print(f\"    Dropped {len(constant_columns)} constant columns\")\n",
    "\n",
    "    # Convert Dask DataFrame to Dask Array\n",
    "    X_train_array = X_train.to_dask_array(lengths=True)\n",
    "    y_train_array = y_train.to_dask_array(lengths=True)\n",
    "    X_test_array = X_test.to_dask_array(lengths=True)\n",
    "    y_test_array = y_test.to_dask_array(lengths=True)\n",
    "    \n",
    "    print(f\"Converting to Dask Array done in {time.time() - new_start_time} seconds ({time.time() - start_time})\")\n",
    "    new_start_time = time.time()\n",
    "\n",
    "    # Standardize the train data and apply the same transformation to the test data\n",
    "    mean = X_train_array.mean(axis=0)\n",
    "    std = X_train_array.std(axis=0)\n",
    "    X_train_array = (X_train_array - mean) / std\n",
    "    X_test_array = (X_test_array - mean) / std\n",
    "    \n",
    "    print(f\"Standardizing data done in {time.time() - new_start_time} seconds ({time.time() - start_time})\")\n",
    "\n",
    "    return X_train_array, y_train_array, X_test_array, y_test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal train-test split done in 0.009440183639526367 seconds (0.009447813034057617)\n",
      "Dropping columns done in 0.013251543045043945 seconds (0.02366471290588379)\n",
      "Persisting data done in 0.04300737380981445 seconds (0.0666964054107666)\n",
      "Dropping constant columns done in 42.125470876693726 seconds (42.19220018386841)\n",
      "    Dropped 0 constant columns\n",
      "Converting to Dask Array done in 0.21129226684570312 seconds (42.40859794616699)\n",
      "Standardizing data done in 0.011327981948852539 seconds (42.41996669769287)\n"
     ]
    }
   ],
   "source": [
    "# make the train-test split\n",
    "start_time = time.time()\n",
    "X_train, y_train, X_test, y_test = train_test_split(data)\n",
    "# write to results_comparison.txt\n",
    "with open('results_comparison.txt', 'a') as f:\n",
    "    f.write(f\"{'Train-test split':<35}{time.time() - start_time:<10.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "start_time = time.time()\n",
    "lr.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "lr_fit_time = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "end_time = time.time()\n",
    "lr_predict_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: XGBoost\n",
    "dtrain = xgb.DMatrix(X_train.compute(), label=y_train.compute())\n",
    "dtest = xgb.DMatrix(X_test.compute(), label=y_test.compute())\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "end_time = time.time()\n",
    "xgb_fit_time = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_xgb = bst.predict(dtest)\n",
    "end_time = time.time()\n",
    "xgb_predict_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for training\n",
    "batch_size = 100\n",
    "\n",
    "# Model 1: SGDRegressor\n",
    "sgd = SGDRegressor()\n",
    "incremental_sgd = Incremental(sgd)\n",
    "\n",
    "# Fit the model in batches\n",
    "start_time = time.time()\n",
    "for i in range(0, len(X_train), batch_size):\n",
    "    end = i + batch_size\n",
    "    X_batch = X_train[i:end].compute()\n",
    "    y_batch = y_train[i:end].compute()\n",
    "    incremental_sgd.partial_fit(X_batch, y_batch)\n",
    "end_time = time.time()\n",
    "sgd_fit_time = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_sgd = incremental_sgd.predict(X_test.compute())\n",
    "end_time = time.time()\n",
    "sgd_predict_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all times to results_comparison.txt\n",
    "with open('results_comparison.txt', 'a') as f:\n",
    "    f.write(f\"{'Linear Regression fit':<35}{lr_fit_time:<10.2f}\\n\")\n",
    "    f.write(f\"{'Linear Regression predict':<35}{lr_predict_time:<10.2f}\\n\")\n",
    "    f.write(f\"{'XGBoost fit':<35}{xgb_fit_time:<10.2f}\\n\")\n",
    "    f.write(f\"{'XGBoost predict':<35}{xgb_predict_time:<10.2f}\\n\")\n",
    "    f.write(f\"{'SGD fit':<35}{sgd_fit_time:<10.2f}\\n\")\n",
    "    f.write(f\"{'SGD predict':<35}{sgd_predict_time:<10.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preditc the mean for all days\n",
    "y_pred_mean = np.full(len(y_test), y_train.mean().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'RMSE': root_mean_squared_error(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: {'MAE': 293.3383467851435, 'RMSE': 868.1107518204351}\n"
     ]
    }
   ],
   "source": [
    "results_baseline = evaluate_model(y_test.compute(), y_pred_mean)\n",
    "print('Baseline:', results_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'RMSE': root_mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "start_time = time.time()\n",
    "results_lr = evaluate_model(y_test.compute(), y_pred_lr.compute())\n",
    "time_lr = time.time() - start_time\n",
    "start_time = time.time()\n",
    "results_xgb = evaluate_model(y_test.compute(), y_pred_xgb)\n",
    "time_xgb = time.time() - start_time\n",
    "start_time = time.time()\n",
    "results_sgd = evaluate_model(y_test.compute(), y_pred_sgd)\n",
    "time_sgd = time.time() - start_time\n",
    "\n",
    "with open('results_comparison.txt', 'a') as f:\n",
    "    f.write(f\"{'Linear regression evaluation':<35} {time_lr:<10.2f}\\n\")\n",
    "    f.write(f\"{'XGBoost evaluation':<35} {time_xgb:<10.2f}\\n\")\n",
    "    f.write(f\"{'SGD regression evaluation':<35} {time_sgd:<10.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method              MAE                      RMSE                     fit time [ms]  predict time [ms]\n",
      "Linear Regression   332.42                   859.46                   53.20          0.00           \n",
      "XGBoost             351.87                   864.64                   1.10           0.03           \n",
      "SGDRegressor        337.68                   857.57                   169.48         0.17           \n",
      "____________________________________________________________________________________________________\n",
      "Baseline            293.34                   868.11                   0              0              \n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(f\"{'Method':<20}{'MAE':<25}{'RMSE':<25}{'fit time [ms]':<15}{'predict time [ms]':<15}\")\n",
    "print(f\"{f'Linear Regression':<20}{results_lr['MAE']:<25.2f}{results_lr['RMSE']:<25.2f}{lr_fit_time:<15.2f}{lr_predict_time:<15.2f}\")\n",
    "print(f\"{f'XGBoost':<20}{results_xgb['MAE']:<25.2f}{results_xgb['RMSE']:<25.2f}{xgb_fit_time:<15.2f}{xgb_predict_time:<15.2f}\")\n",
    "print(f\"{f'SGDRegressor':<20}{results_sgd['MAE']:<25.2f}{results_sgd['RMSE']:<25.2f}{sgd_fit_time:<15.2f}{sgd_predict_time:<15.2f}\")\n",
    "print(\"_\"*100)\n",
    "print(f\"{'Baseline':<20}{results_baseline['MAE']:<25.2f}{results_baseline['RMSE']:<25.2f}{0:<15}{0:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for different counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path pattern to read all CSV files (adjust the path as needed)\n",
    "file_pattern = 'nycweather_pesjak/nyc_w_*.csv'\n",
    "\n",
    "# Specify the data types for each column\n",
    "dtypes = {\n",
    "    'datetime': 'str',  # 'datetime' will be parsed separately\n",
    "    'temp': 'float64',\n",
    "    'feelslike': 'float64',\n",
    "    'dew': 'float64',\n",
    "    'humidity': 'float64',\n",
    "    'precip': 'float64',\n",
    "    'precipprob': 'float64',\n",
    "    'preciptype': 'object',  # String data type\n",
    "    'snow': 'float64',\n",
    "    'snowdepth': 'float64',\n",
    "    'windgust': 'float64',\n",
    "    'windspeed': 'float64',\n",
    "    'winddir': 'float64',\n",
    "    'sealevelpressure': 'float64',\n",
    "    'cloudcover': 'float64',\n",
    "    'visibility': 'float64',\n",
    "    'solarradiation': 'float64',\n",
    "    'solarenergy': 'float64',\n",
    "    'uvindex': 'float64',\n",
    "    'severerisk': 'float64',\n",
    "    'conditions': 'object',  # String data type\n",
    "    'icon': 'object',  # String data type\n",
    "    'stations': 'object'  # String data type\n",
    "}\n",
    "\n",
    "# Read all CSV files into a single Dask DataFrame with specified dtypes\n",
    "weather_df = dd.read_csv(file_pattern, dtype=dtypes)\n",
    "\n",
    "# Convert the 'datetime' column to datetime type\n",
    "weather_df['datetime'] = dd.to_datetime(weather_df['datetime'], format='%Y-%m-%dT%H:%M:%S', errors='coerce')\n",
    "\n",
    "# Drop all string columns except 'datetime'\n",
    "columns_to_keep = ['datetime'] + [col for col in weather_df.columns if weather_df[col].dtype in ['float64', 'int64']]\n",
    "weather_df = weather_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data_paths = [\"holiday_counts_by_hour\", \"landmark_counts_by_hour\", \"school_counts_by_hour\", \"business_counts_by_hour\", \"event_counts_by_hour\"]\n",
    "\n",
    "for path in additional_data_paths:\n",
    "    # Load the data\n",
    "    additional_data = dd.read_csv(path+\".csv\")\n",
    "    \n",
    "    # convert the datetime column to datetime type\n",
    "    additional_data['datetime'] = dd.to_datetime(additional_data['datetime'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    # Merge the data\n",
    "    weather_df = dd.merge(weather_df, additional_data, on='datetime', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime                                 datetime64[ns]\n",
      "temp                                     float64\n",
      "feelslike                                float64\n",
      "dew                                      float64\n",
      "humidity                                 float64\n",
      "precip                                   float64\n",
      "precipprob                               float64\n",
      "snow                                     float64\n",
      "snowdepth                                float64\n",
      "windgust                                 float64\n",
      "windspeed                                float64\n",
      "winddir                                  float64\n",
      "sealevelpressure                         float64\n",
      "cloudcover                               float64\n",
      "visibility                               float64\n",
      "solarradiation                           float64\n",
      "solarenergy                              float64\n",
      "uvindex                                  float64\n",
      "severerisk                               float64\n",
      "national_holiday                         int64\n",
      "religious_holiday                        int64\n",
      "special_day                              int64\n",
      "school_holiday                           int64\n",
      "NumLandmarksOpened                       int64\n",
      "Sum_Shape_Leng                           float64\n",
      "Sum_Shape_Area                           float64\n",
      "Elementary                               int64\n",
      "High school                              int64\n",
      "Junior High-Intermediate-Middle          int64\n",
      "K-8                                      int64\n",
      "Secondary School                         int64\n",
      "K-12 all grades                          int64\n",
      "Early Childhood                          int64\n",
      "Ungraded                                 int64\n",
      "Collaborative or Multi-graded            int64\n",
      "LicenseType_Individual                   int64\n",
      "LicenseType_Business                     int64\n",
      "Industry_Sightseeing Guide               int64\n",
      "Industry_Ticket Seller                   int64\n",
      "Industry_Locksmith                       int64\n",
      "Industry_Electronic & Appliance Service  int64\n",
      "Industry_General Vendor                  int64\n",
      "Industry_Employment Agency               int64\n",
      "Industry_Electronic Cigarette Dealer     int64\n",
      "Industry_Newsstand                       int64\n",
      "Industry_Pedicab Driver                  int64\n",
      "Industry_Tobacco Retail Dealer           int64\n",
      "Industry_Garage                          int64\n",
      "Industry_Stoop Line Stand                int64\n",
      "Industry_Secondhand Dealer - General     int64\n",
      "Industry_Tow Truck Driver                int64\n",
      "Industry_Laundries                       int64\n",
      "Industry_Electronics Store               int64\n",
      "Industry_Process Server Individual       int64\n",
      "Industry_Home Improvement Contractor     int64\n",
      "Industry_Dealer In Products              int64\n",
      "Industry_Tow Truck Company               int64\n",
      "Industry_Debt Collection Agency          int64\n",
      "Industry_Process Serving Agency          int64\n",
      "Industry_Horse Drawn Driver              int64\n",
      "Industry_Car Wash                        int64\n",
      "Industry_Scale Dealer Repairer           int64\n",
      "Industry_Construction Labor Provider     int64\n",
      "Industry_Third Party Food Delivery       int64\n",
      "Industry_Secondhand Dealer - Auto        int64\n",
      "Industry_Pawnbroker                      int64\n",
      "Industry_Pedicab Business                int64\n",
      "Industry_Parking Lot                     int64\n",
      "Industry_Bingo Game Operator             int64\n",
      "Industry_Garage and Parking Lot          int64\n",
      "Industry_Horse Drawn Cab Owner           int64\n",
      "Industry_Scrap Metal Processor           int64\n",
      "Industry_Games of Chance                 int64\n",
      "Industry_Booting Company                 int64\n",
      "Industry_Storage Warehouse               int64\n",
      "Industry_Sightseeing Bus                 int64\n",
      "Industry_Ticket Seller Business          int64\n",
      "Industry_Locksmith Apprentice            int64\n",
      "Industry_Commercial Lessor               int64\n",
      "Industry_General Vendor Distributor      int64\n",
      "Industry_Sidewalk Cafe                   int64\n",
      "EventType_Special Event                  int64\n",
      "EventType_Farmers Market                 int64\n",
      "EventType_Construction                   int64\n",
      "EventType_Sidewalk Sale                  int64\n",
      "EventType_Parade                         int64\n",
      "EventType_Plaza Event                    int64\n",
      "EventType_Street Event                   int64\n",
      "EventType_Shooting Permit                int64\n",
      "EventType_Production Event               int64\n",
      "EventType_Street Festival                int64\n",
      "EventType_Theater Load in and Load Outs  int64\n",
      "EventType_Block Party                    int64\n",
      "EventType_Plaza Partner Event            int64\n",
      "EventType_Press Conference               int64\n",
      "EventType_Religious Event                int64\n",
      "EventType_Clean-Up                       int64\n",
      "EventType_Athletic Race / Tour           int64\n",
      "EventType_Miscellaneous                  int64\n",
      "EventType_Stickball                      int64\n",
      "EventType_Single Block Festival          int64\n",
      "EventType_Public Program / Exhibitions   int64\n",
      "EventType_Weekend Walk                   int64\n",
      "EventType_Rigging Permit                 int64\n",
      "EventType_Sport - Youth                  int64\n",
      "EventType_Sport - Adult                  int64\n",
      "EventType_Rally                          int64\n",
      "EventType_Stationary Demonstration       int64\n",
      "EventType_Health Fair                    int64\n",
      "EventType_Mobile Unit                    int64\n",
      "EventType_Commercial Promotional Event   int64\n",
      "EventType_Athletic                       int64\n",
      "EventType_Filming/Photography            int64\n",
      "EventType_Play Streets                   int64\n",
      "EventType_Concert                        int64\n",
      "EventType_Press Conference Media         int64\n",
      "EventType_Marathon                       int64\n",
      "EventType_nan                            int64\n",
      "EventType_Film Shoot / Production        int64\n",
      "EventType_DCAS Prep/Shoot/Wrap Permit    int64\n",
      "EventType_Red Carpet Event               int64\n",
      "EventType_Urban Art Program              int64\n",
      "EventType_Embargo                        int64\n",
      "EventType_Open Culture                   int64\n",
      "EventType_Theater Load in and Load Outs  int64\n",
      "EventType_Athletic - Charitable          int64\n",
      "EventType_Open Street Partner Event      int64\n",
      "EventType_Open Street Event              int64\n",
      "EventType_Bike the Block                 int64\n",
      "EventType_BID Multi-Block                int64\n",
      "EventType_Grid Request                   int64\n",
      "NumEvents                                int64\n",
      "NumConstructions                         int64\n",
      "NumStreetClosures                        int64\n"
     ]
    }
   ],
   "source": [
    "for column in weather_df.columns:\n",
    "    print(f\"{column:<40} {weather_df[column].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_corona_time(data):\n",
    "    start_date = '2020-01-30 00:00:00'\n",
    "    end_date = '2023-05-5 23:00:00'\n",
    "\n",
    "    # Convert 'datetime' column to datetime using map_partitions\n",
    "    data['datetime'] = data.map_partitions(lambda df: pd.to_datetime(df['datetime']), meta=('datetime', 'datetime64[ns]'))\n",
    "\n",
    "    # Apply the condition to set 'corona_time' across partitions\n",
    "    data['corona_time'] = data.map_partitions(\n",
    "        lambda df: ((df['datetime'] >= start_date) & (df['datetime'] <= end_date)).astype(int),\n",
    "        meta=('corona_time', 'int')\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_func(X_train, y_train, X_test, y_test):\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    start_time = time.time()\n",
    "    lr.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    lr_fit_time = end_time - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    lr_predict_time = end_time - start_time\n",
    "\n",
    "    results_lr = evaluate_model(y_test.compute(), y_pred_lr.compute())\n",
    "    \n",
    "    results_dict = {\n",
    "        'MAE': results_lr['MAE'],\n",
    "        'RMSE': results_lr['RMSE'],\n",
    "        'fit_time': lr_fit_time,\n",
    "        'predict_time': lr_predict_time\n",
    "    }\n",
    "    \n",
    "    return results_dict\n",
    "    \n",
    "def xgb_func(X_train, y_train, X_test, y_test):\n",
    "    dtrain = xgb.DMatrix(X_train.compute(), label=y_train.compute())\n",
    "    dtest = xgb.DMatrix(X_test.compute(), label=y_test.compute())\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'rmse'\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "    end_time = time.time()\n",
    "    xgb_fit_time = end_time - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_pred_xgb = bst.predict(dtest)\n",
    "    end_time = time.time()\n",
    "    xgb_predict_time = end_time - start_time\n",
    "\n",
    "    results_xgb = evaluate_model(y_test.compute(), y_pred_xgb)\n",
    "    \n",
    "    results_dict = {\n",
    "        'MAE': results_xgb['MAE'],\n",
    "        'RMSE': results_xgb['RMSE'],\n",
    "        'fit_time': xgb_fit_time,\n",
    "        'predict_time': xgb_predict_time\n",
    "    }\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def sgd_func(X_train, y_train, X_test, y_test):\n",
    "    batch_size = 100\n",
    "    sgd = SGDRegressor()\n",
    "    incremental_sgd = Incremental(sgd)\n",
    "\n",
    "    # Fit the model in batches\n",
    "    start_time = time.time()\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        end = i + batch_size\n",
    "        X_batch = X_train[i:end].compute()\n",
    "        y_batch = y_train[i:end].compute()\n",
    "        incremental_sgd.partial_fit(X_batch, y_batch)\n",
    "    end_time = time.time()\n",
    "    sgd_fit_time = end_time - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_pred_sgd = incremental_sgd.predict(X_test.compute())\n",
    "    end_time = time.time()\n",
    "    sgd_predict_time = end_time - start_time\n",
    "\n",
    "    results_sgd = evaluate_model(y_test.compute(), y_pred_sgd)\n",
    "    \n",
    "    results_dict = {\n",
    "        'MAE': results_sgd['MAE'],\n",
    "        'RMSE': results_sgd['RMSE'],\n",
    "        'fit_time': sgd_fit_time,\n",
    "        'predict_time': sgd_predict_time\n",
    "    }\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def mean_func(X_train, y_train, X_test, y_test):\n",
    "    y_pred_mean = np.full(len(y_test), y_train.mean().compute())\n",
    "    results_mean = evaluate_model(y_test.compute(), y_pred_mean)\n",
    "    \n",
    "    results_dict = {\n",
    "        'MAE': results_mean['MAE'],\n",
    "        'RMSE': results_mean['RMSE'],\n",
    "        'fit_time': 0,\n",
    "        'predict_time': 0\n",
    "    }\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def print_results(results, county):\n",
    "    print(\"_\"*90)\n",
    "    print(f\"{county}\")\n",
    "    print(f\"{'Method':<20}{'MAE':<15}{'RMSE':<15}{'fit time [ms]':<15}{'predict time [ms]':<15}\")\n",
    "    for key, value in results.items():\n",
    "        print(f\"{key:<20}{value['MAE']:<15.2f}{value['RMSE']:<15.2f}{value['fit_time']:<15.2f}{value['predict_time']:<15.2f}\")\n",
    "    print(\"_\"*90)\n",
    "    \n",
    "def save_results(results, county, save_to=\"results.txt\"):\n",
    "    with open(save_to, 'a') as f:\n",
    "        f.write(\"_\"*90 + \"\\n\")\n",
    "        f.write(f\"{county}\\n\")\n",
    "        f.write(f\"{'Method':<20}{'MAE':<15}{'RMSE':<15}{'fit time [ms]':<15}{'predict time [ms]':<15}\\n\")\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key:<20}{value['MAE']:<15.2f}{value['RMSE']:<15.2f}{value['fit_time']:<15.2f}{value['predict_time']:<15.2f}\\n\")\n",
    "        f.write(\"_\"*90 + \"\\n\")\n",
    "        \n",
    "def format_global_time(start_time):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return str(datetime.timedelta(seconds=elapsed_time))\n",
    "\n",
    "def format_internal_time(start_time):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time < 1:\n",
    "        return str(round(elapsed_time * 1000, 2)) + \" ms\"\n",
    "    elif elapsed_time < 60:\n",
    "        return str(round(elapsed_time, 2)) + \" s\"\n",
    "    elif elapsed_time < 3600:\n",
    "        return str(elapsed_time // 60) + \" min and \" + str(round(elapsed_time % 60, 2)) + \" s\"\n",
    "    else:\n",
    "        return str(elapsed_time // 3600) + \" h and \" + str(round((elapsed_time % 3600) / 60, 2)) + \" min\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0:00:00.304759) Processing Manhattan\n",
      "(0:00:00.434199)  Data cleaning done in 129.46 ms\n",
      "(0:00:00.518110)  Data preparation done in 83.87 ms\n",
      "Taking data from 2013-01-01 02:00:00 to 2024-12-26 06:00:00\n",
      "(0:01:39.292950)  Filling in hours done in 1.0 min and 38.77 s\n",
      "(0:01:39.403507)  Adding weather data done in 110.52 ms\n",
      "Temporal train-test split done in 0.0073490142822265625 seconds (0.007355928421020508)\n",
      "Dropping columns done in 0.036359548568725586 seconds (0.04374217987060547)\n",
      "Persisting data done in 0.0928957462310791 seconds (0.13679099082946777)\n",
      "Dropping constant columns done in 53.98084831237793 seconds (54.117680072784424)\n",
      "    Dropped 12 constant columns\n",
      "Converting to Dask Array done in 0.43853759765625 seconds (54.556354999542236)\n",
      "Standardizing data done in 0.008935213088989258 seconds (54.56557250022888)\n",
      "(0:02:33.971918)  Train-test split done in 54.57 s\n",
      "(0:04:46.072552)  Linear Regression done in 2.0 min and 12.1 s\n",
      "(0:04:50.555971)  XGBoost done in 4.48 s\n",
      "(0:05:42.333490)  SGDRegressor done in 51.78 s\n",
      "(0:05:42.486755)  Baseline done in 152.9 ms\n",
      "__________________________________________________________________________________________\n",
      "Manhattan\n",
      "Method              MAE            RMSE           fit time [ms]  predict time [ms]\n",
      "Linear Regression   154.96         280.76         131.95         0.01           \n",
      "XGBoost             401.52         519.90         3.81           0.03           \n",
      "SGDRegressor        93916.42       134202.54      51.69          0.06           \n",
      "Baseline            83.19          255.34         0.00           0.00           \n",
      "__________________________________________________________________________________________\n",
      "\n",
      "(0:05:42.586555) Processing Queens\n",
      "(0:05:42.770869)  Data cleaning done in 184.34 ms\n",
      "(0:05:42.849817)  Data preparation done in 78.9 ms\n",
      "Taking data from 2013-01-01 01:00:00 to 2024-12-31 00:00:00\n",
      "(0:06:53.756331)  Filling in hours done in 1.0 min and 10.91 s\n",
      "(0:06:53.878273)  Adding weather data done in 121.9 ms\n",
      "Temporal train-test split done in 0.007777214050292969 seconds (0.0077860355377197266)\n",
      "Dropping columns done in 0.03618741035461426 seconds (0.04400134086608887)\n",
      "Persisting data done in 0.08333253860473633 seconds (0.1273648738861084)\n",
      "Dropping constant columns done in 53.80594825744629 seconds (53.933354139328)\n",
      "    Dropped 12 constant columns\n",
      "Converting to Dask Array done in 0.4729452133178711 seconds (54.40642070770264)\n",
      "Standardizing data done in 0.014425992965698242 seconds (54.42311978340149)\n",
      "(0:07:48.302679)  Train-test split done in 54.42 s\n",
      "(0:10:22.971738)  Linear Regression done in 2.0 min and 34.67 s\n",
      "(0:10:27.458500)  XGBoost done in 4.49 s\n",
      "(0:11:21.085140)  SGDRegressor done in 53.63 s\n",
      "(0:11:21.186467)  Baseline done in 101.11 ms\n",
      "__________________________________________________________________________________________\n",
      "Queens\n",
      "Method              MAE            RMSE           fit time [ms]  predict time [ms]\n",
      "Linear Regression   202.52         309.26         130.68         0.00           \n",
      "XGBoost             241.64         344.45         3.73           0.03           \n",
      "SGDRegressor        25069.53       35970.06       53.53          0.06           \n",
      "Baseline            82.39          252.42         0.00           0.00           \n",
      "__________________________________________________________________________________________\n",
      "\n",
      "(0:11:21.235373) Processing Brooklyn\n",
      "(0:11:21.377141)  Data cleaning done in 141.79 ms\n",
      "(0:11:21.450984)  Data preparation done in 73.05 ms\n",
      "Taking data from 2013-01-01 02:00:00 to 2024-12-31 17:00:00\n",
      "(0:12:32.861418)  Filling in hours done in 1.0 min and 11.41 s\n",
      "(0:12:32.968547)  Adding weather data done in 107.09 ms\n",
      "Temporal train-test split done in 0.0071811676025390625 seconds (0.007187843322753906)\n",
      "Dropping columns done in 0.03291916847229004 seconds (0.04013180732727051)\n",
      "Persisting data done in 0.07769942283630371 seconds (0.1178596019744873)\n",
      "Dropping constant columns done in 54.39807391166687 seconds (54.5159707069397)\n",
      "    Dropped 12 constant columns\n",
      "Converting to Dask Array done in 0.4447307586669922 seconds (54.961527824401855)\n",
      "Standardizing data done in 0.020041465759277344 seconds (54.98207497596741)\n",
      "(0:13:27.952305)  Train-test split done in 54.98 s\n",
      "(0:16:02.693566)  Linear Regression done in 2.0 min and 34.74 s\n",
      "(0:16:07.329552)  XGBoost done in 4.64 s\n",
      "(0:17:01.208476)  SGDRegressor done in 53.88 s\n",
      "(0:17:01.298160)  Baseline done in 89.2 ms\n",
      "__________________________________________________________________________________________\n",
      "Brooklyn\n",
      "Method              MAE            RMSE           fit time [ms]  predict time [ms]\n",
      "Linear Regression   122.30         244.26         132.01         0.01           \n",
      "XGBoost             235.51         336.29         3.90           0.03           \n",
      "SGDRegressor        148233.79      272198.08      53.79          0.06           \n",
      "Baseline            78.54          234.22         0.00           0.00           \n",
      "__________________________________________________________________________________________\n",
      "\n",
      "(0:17:01.329885) Processing Bronx\n",
      "(0:17:01.482810)  Data cleaning done in 152.95 ms\n",
      "(0:17:01.553946)  Data preparation done in 71.01 ms\n",
      "Taking data from 2013-01-01 00:00:00 to 2024-12-31 21:00:00\n",
      "(0:18:12.197851)  Filling in hours done in 1.0 min and 10.64 s\n",
      "(0:18:12.306437)  Adding weather data done in 108.55 ms\n",
      "Temporal train-test split done in 0.007150888442993164 seconds (0.007159233093261719)\n",
      "Dropping columns done in 0.037767887115478516 seconds (0.0449526309967041)\n",
      "Persisting data done in 0.07311272621154785 seconds (0.11809897422790527)\n",
      "Dropping constant columns done in 53.57008457183838 seconds (53.68822383880615)\n",
      "    Dropped 12 constant columns\n",
      "Converting to Dask Array done in 0.3881387710571289 seconds (54.076855421066284)\n",
      "Standardizing data done in 0.012946367263793945 seconds (54.08995819091797)\n",
      "(0:19:06.398532)  Train-test split done in 54.09 s\n",
      "(0:21:19.995626)  Linear Regression done in 2.0 min and 13.6 s\n",
      "(0:21:24.496757)  XGBoost done in 4.5 s\n",
      "(0:22:17.738831)  SGDRegressor done in 53.24 s\n",
      "(0:22:17.848271)  Baseline done in 109.31 ms\n",
      "__________________________________________________________________________________________\n",
      "Bronx\n",
      "Method              MAE            RMSE           fit time [ms]  predict time [ms]\n",
      "Linear Regression   91.98          139.30         133.36         0.00           \n",
      "XGBoost             107.33         150.39         3.84           0.03           \n",
      "SGDRegressor        85860.45       141258.47      53.16          0.06           \n",
      "Baseline            38.50          110.28         0.00           0.00           \n",
      "__________________________________________________________________________________________\n",
      "\n",
      "(0:22:17.924227) Processing Staten Island\n",
      "(0:22:18.049294)  Data cleaning done in 125.1 ms\n",
      "(0:22:18.116557)  Data preparation done in 67.15 ms\n",
      "Taking data from 2013-01-01 09:00:00 to 2024-12-22 06:00:00\n",
      "(0:23:35.572431)  Filling in hours done in 1.0 min and 17.46 s\n",
      "(0:23:35.677284)  Adding weather data done in 104.81 ms\n",
      "Temporal train-test split done in 0.007137298583984375 seconds (0.0071451663970947266)\n",
      "Dropping columns done in 0.03571748733520508 seconds (0.042887210845947266)\n",
      "Persisting data done in 0.07280802726745605 seconds (0.11572861671447754)\n",
      "Dropping constant columns done in 57.39535164833069 seconds (57.511117458343506)\n",
      "    Dropped 12 constant columns\n",
      "Converting to Dask Array done in 0.3624098300933838 seconds (57.87375235557556)\n",
      "Standardizing data done in 0.00802159309387207 seconds (57.881991386413574)\n",
      "(0:24:33.561317)  Train-test split done in 57.88 s\n",
      "(0:26:52.163665)  Linear Regression done in 2.0 min and 18.6 s\n",
      "(0:26:56.588482)  XGBoost done in 4.42 s\n",
      "(0:27:50.482859)  SGDRegressor done in 53.89 s\n",
      "(0:27:50.552868)  Baseline done in 68.59 ms\n",
      "__________________________________________________________________________________________\n",
      "Staten Island\n",
      "Method              MAE            RMSE           fit time [ms]  predict time [ms]\n",
      "Linear Regression   30.97          47.10          138.45         0.01           \n",
      "XGBoost             30.97          47.41          3.66           0.03           \n",
      "SGDRegressor        3478.99        4141.71        53.75          0.12           \n",
      "Baseline            10.82          36.14          0.00           0.00           \n",
      "__________________________________________________________________________________________\n",
      "\n",
      "(0:27:50.580487) Processing all\n",
      "(0:27:50.765309)  Data cleaning done in 184.84 ms\n",
      "(0:27:50.840451)  Data preparation done in 74.99 ms\n",
      "Taking data from 2013-01-01 00:00:00 to 2024-12-31 21:00:00\n",
      "(0:29:07.894811)  Filling in hours done in 1.0 min and 17.05 s\n",
      "(0:29:08.029943)  Adding weather data done in 135.09 ms\n",
      "Temporal train-test split done in 0.007336139678955078 seconds (0.0073430538177490234)\n",
      "Dropping columns done in 0.05128335952758789 seconds (0.05865311622619629)\n",
      "Persisting data done in 0.07653927803039551 seconds (0.13522887229919434)\n",
      "Dropping constant columns done in 57.133023500442505 seconds (57.26829504966736)\n",
      "    Dropped 12 constant columns\n",
      "Converting to Dask Array done in 0.388153076171875 seconds (57.65662217140198)\n",
      "Standardizing data done in 0.01539301872253418 seconds (57.67431855201721)\n",
      "(0:30:05.706475)  Train-test split done in 57.68 s\n",
      "(0:32:22.165865)  Linear Regression done in 2.0 min and 16.46 s\n",
      "(0:32:26.551889)  XGBoost done in 4.39 s\n",
      "(0:33:20.027812)  SGDRegressor done in 53.48 s\n",
      "(0:33:20.097397)  Baseline done in 69.44 ms\n",
      "__________________________________________________________________________________________\n",
      "all\n",
      "Method              MAE            RMSE           fit time [ms]  predict time [ms]\n",
      "Linear Regression   553.19         955.18         136.30         0.01           \n",
      "XGBoost             796.16         1190.60        3.65           0.03           \n",
      "SGDRegressor        81394.05       161633.10      53.39          0.06           \n",
      "Baseline            293.34         868.11         0.00           0.00           \n",
      "__________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "\n",
    "data = dd.read_parquet('/d/hpc/projects/FRI/bigdata/students/dp8949/parquet_data/*.parquet')\n",
    "\n",
    "clean_data = data_cleaning(data).persist()\n",
    "\n",
    "counties = [\"Manhattan\", \"Queens\", \"Brooklyn\", \"Bronx\", \"Staten Island\", \"all\"]\n",
    "# counties = [\"all\"]\n",
    "methods = {\"Linear Regression\": lr_func, \"XGBoost\": xgb_func, \"SGDRegressor\": sgd_func, \"Baseline\": mean_func}\n",
    "# methods = {\"XGBoost\": xgb_func, \"Baseline\": mean_func}\n",
    "\n",
    "for county in counties:\n",
    "    internal_start_time = time.time()\n",
    "    print(f\"\\n({format_global_time(global_start_time)}) Processing {county}\")\n",
    "    county_data = data_cleaning(data)\n",
    "    \n",
    "    print(f\"({format_global_time(global_start_time)})  Data cleaning done in {format_internal_time(internal_start_time)}\")\n",
    "    internal_start_time = time.time()\n",
    "    \n",
    "    if county != 'all':\n",
    "        county_data = county_data[county_data['violation_county'] == county]\n",
    "        # drop the Violation County column\n",
    "        county_data = county_data.drop(columns=['violation_county'])\n",
    "    else:\n",
    "        # drop the Violation County column\n",
    "        county_data = county_data.drop(columns=['violation_county'])\n",
    "\n",
    "    county_data = county_data.groupby(['violation_year', 'violation_month', 'violation_day_month', 'violation_day_week', 'violation_hour']).size().reset_index()\n",
    "    county_data = county_data.rename(columns={0: 'violation_count'})\n",
    "\n",
    "    county_data['datetime'] = county_data['violation_year'].astype(int).astype(str) + '-' + \\\n",
    "                              county_data['violation_month'].astype(int).astype(str).str.zfill(2) + '-' + \\\n",
    "                              county_data['violation_day_month'].astype(int).astype(str).str.zfill(2) + ' ' + \\\n",
    "                              county_data['violation_hour'].astype(int).astype(str).str.zfill(2) + ':00:00'\n",
    "    county_data = county_data.rename(columns={0: 'violation_count'})  \n",
    "\n",
    "    # county_data['datetime'] = county_data['violation_year'].astype(str) + '-' + county_data['violation_month'].astype(str).str.zfill(2) + '-' + county_data['violation_day_month'].astype(str).str.zfill(2) + ' ' + county_data['violation_hour'].astype(str).str.zfill(2) + ':00:00'\n",
    "    county_data['datetime'] = dd.to_datetime(county_data['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # county_data = county_data[(county_data['datetime'] >= '2013-01-01') & (county_data['datetime'] <= '2015-03-31')]\n",
    "    \n",
    "    print(f\"({format_global_time(global_start_time)})  Data preparation done in {format_internal_time(internal_start_time)}\")\n",
    "    internal_start_time = time.time()\n",
    "    \n",
    "    # fill in the missing hours\n",
    "    county_data = fill_in_hours(county_data)\n",
    "    print(f\"({format_global_time(global_start_time)})  Filling in hours done in {format_internal_time(internal_start_time)}\")\n",
    "    internal_start_time = time.time()\n",
    "    \n",
    "    # add weather data\n",
    "    county_data = add_weather_data(county_data, weather_df)\n",
    "    print(f\"({format_global_time(global_start_time)})  Adding weather data done in {format_internal_time(internal_start_time)}\")\n",
    "    internal_start_time = time.time()\n",
    "    \n",
    "    # make the train-test split\n",
    "    X_train, y_train, X_test, y_test = train_test_split(county_data)\n",
    "    print(f\"({format_global_time(global_start_time)})  Train-test split done in {format_internal_time(internal_start_time)}\")\n",
    "    internal_start_time = time.time()\n",
    "\n",
    "    # persist the data\n",
    "    X_train, y_train, X_test, y_test = client.persist([X_train, y_train, X_test, y_test])\n",
    "    \n",
    "    # evaluate the models\n",
    "    results = {}\n",
    "    for method in methods:\n",
    "        results[method] = methods[method](X_train, y_train, X_test, y_test)\n",
    "        print(f\"({format_global_time(global_start_time)})  {method} done in {format_internal_time(internal_start_time)}\")\n",
    "        internal_start_time = time.time()\n",
    "        \n",
    "    \n",
    "    # clear all data\n",
    "    del county_data\n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    \n",
    "    print_results(results, county)\n",
    "    save_results(results, county)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'county_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcounty_data\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'county_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(county_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bd39)",
   "language": "python",
   "name": "bd39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
